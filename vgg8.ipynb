{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR NAMES HERE**\n",
    "\n",
    "Spring 2025\n",
    "\n",
    "CS 444: Deep Learning\n",
    "\n",
    "Project 1: Deep Neural Networks \n",
    "\n",
    "#### Week 3: Strategies for training deeper networks\n",
    "\n",
    "The focus this week is on strategies for training deeper neural networks. We will touch on:\n",
    "1. Improved weight initialization techniques.\n",
    "2. Using dropout.\n",
    "3. Using regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6c. Add support for He/Kaiming initialization\n",
    "\n",
    "Continuing on with our goal of improving our training workflow, let's implement He/Kaiming initialization weight initialization in layers that contain weights (`'he'` method). We want to maintain support for the current way weights are initialized (`'normal'` method) so that we can switch between the method used to initialize the weights. In summary, the process involves a) adding support for `'he'` weight initialization in layers with weights and b) adding keyword arguments to allow switching between the weight initialization methods.\n",
    "\n",
    "Here are the changes to make:\n",
    "1. In `Layer`, implement `get_kaiming_gain` to get the activation function dependent Kaiming gain $\\kappa$.\n",
    "2. If you have not already done so, add instance vars for the  `wt_init` parameter in:\n",
    "   1. `Dense` constructor\n",
    "   2. `Conv2D` constructor\n",
    "   3. `VGG4` constructor\n",
    "   4. `VGGConvBlock` constructor\n",
    "   5. `VGGDenseBlock` constructor\n",
    "   6. `VGG6` constructor\n",
    "3. In the following places, initialize the weights and bias in the way you are currently are if using the `'normal'` the weight initialization method (*the default in all layers so far*). Otherwise, use He/Kaiming:\n",
    "   1. `Dense` `init_params`\n",
    "   2. `Conv2D` `init_params`\n",
    "4. Make sure you are passing the `wt_init` parameter setting along when making layers/blocks in the following places:\n",
    "   1. `VGGConvBlock`\n",
    "   2. `VGGDenseBlock`\n",
    "   3. `VGG4`\n",
    "   4. `VGG6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import Dense, Conv2D\n",
    "from block import VGGConvBlock, VGGDenseBlock\n",
    "from vgg_nets import VGG4, VGG6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `Dense`\n",
    "\n",
    "Also re-run previous tests in the `build_deeplib` notebook to make sure adding support for He/Kaiming initialization does not affect `'normal'` initialzation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "d = Dense('Test', units=3, wt_init='he')\n",
    "d(tf.ones([2, 4]))\n",
    "print(f'Your wts are\\n{d.get_wts().numpy()} and should be')\n",
    "print('''[[ 1.068  0.299 -0.297]\n",
    " [-0.733 -0.875  0.333]\n",
    " [-0.01   0.841  0.426]\n",
    " [ 0.424 -0.499 -0.306]]''')\n",
    "print(f'Your biases are\\n{d.get_b().numpy()} and should be')\n",
    "print('[0. 0. 0.]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `Conv2D`\n",
    "\n",
    "Also re-run previous tests in the `build_deeplib` notebook to make sure adding support for He/Kaiming initialization does not affect `'normal'` initialzation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "c = Conv2D('Test', units=3, wt_init='he', kernel_size=(2,2))\n",
    "c(tf.ones([1, 6, 6, 2]))\n",
    "print(f'Your wts are\\n{c.get_wts().numpy()} and should be')\n",
    "print('''[[[[ 0.756  0.211 -0.21 ]\n",
    "   [-0.518 -0.618  0.235]]\n",
    "\n",
    "  [[-0.007  0.594  0.301]\n",
    "   [ 0.3   -0.353 -0.216]]]\n",
    "\n",
    "\n",
    " [[[ 0.397 -0.349 -0.48 ]\n",
    "   [-0.45  -0.18  -0.112]]\n",
    "\n",
    "  [[ 0.152  0.261  0.078]\n",
    "   [ 0.794 -0.398  0.039]]]]''')\n",
    "print(f'Your biases are\\n{c.get_b().numpy()} and should be')\n",
    "print('[0. 0. 0.]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `VGGConvBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "v = VGGConvBlock('Test', units=3, wt_init='he', prev_layer_or_block=None)\n",
    "tf.random.set_seed(1)\n",
    "net_acts = v(tf.random.uniform([1, 6, 6, 2]))\n",
    "\n",
    "print(f'Your net_acts are\\n{net_acts.numpy()} and should be')\n",
    "print('''[[[[0.539 0.798 0.174]\n",
    "   [1.075 0.599 0.   ]\n",
    "   [0.713 0.7   0.   ]]\n",
    "\n",
    "  [[0.497 1.139 0.191]\n",
    "   [0.826 1.057 0.075]\n",
    "   [0.506 1.095 0.   ]]\n",
    "\n",
    "  [[0.358 0.638 0.168]\n",
    "   [0.547 0.29  0.189]\n",
    "   [0.726 0.337 0.111]]]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `VGGDenseBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "v = VGGDenseBlock('Test', units=(3,), wt_init='he', prev_layer_or_block=None)\n",
    "tf.random.set_seed(1)\n",
    "net_acts = v(tf.random.uniform([1, 4]))\n",
    "\n",
    "print(f'Your net_acts are\\n{net_acts.numpy()} and should be')\n",
    "print('''[[0.565 0.    0.   ]] ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `VGG4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "v = VGG4(C=3, input_feats_shape=(10, 10, 5), wt_init='he')\n",
    "tf.random.set_seed(1)\n",
    "net_acts = v(tf.random.uniform([2, 10, 10, 5]))\n",
    "\n",
    "print(f'Your net_acts are\\n{net_acts.numpy()} and should be')\n",
    "print('''[[0.369 0.382 0.249]\n",
    " [0.37  0.351 0.28 ]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `VGG6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "v = VGG4(C=4, input_feats_shape=(12, 12, 4), wt_init='he')\n",
    "tf.random.set_seed(1)\n",
    "net_acts = v(tf.random.uniform([1, 12, 12, 4]))\n",
    "\n",
    "print(f'Your net_acts are\\n{net_acts.numpy()} and should be')\n",
    "print('''[[0.228 0.107 0.589 0.075]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6d. Compare training VGG6 with Normal and He initialization with early stopping\n",
    "\n",
    "Let's use VGG6 to test how He initialization and early stopping affects training with CIFAR-10. In the cell below, conduct an experiment wherein you train VGG6 repeatedly 6 times will different combinations of the following: \n",
    "- patience `[3, 4, 5]`\n",
    "- Normal vs He\n",
    "\n",
    "After training each net, store/record:\n",
    "1. the network's final test accuracy.\n",
    "2. the number of epochs spent training.\n",
    "3. the history of training loss (averaged across mini-batches, so one per epoch).\n",
    "4. the history of validation loss (checked every epoch).\n",
    "5. the history of validation accuracy (checked every epoch).\n",
    "\n",
    "**Notes:**\n",
    "- Set random seed before creating each net for consistency.\n",
    "- Use `tf.keras.backend.clear_session()` before creating each of the networks to help prevent the memory used from growing.\n",
    "- Some networks may stop very quickly â€” that is ok and expected!\n",
    "\n",
    "This experiment is expected to take 30-60 mins to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function below to plot:\n",
    "1. The training loss history.\n",
    "2. The val loss history.\n",
    "3. The val acc history.\n",
    "\n",
    "Afterwards print the test accuracy for each network along wth the number of actual training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_and_acc_wt_init_exp(hists,\n",
    "                                  labels=['normal3', 'he3', 'normal4', 'he4', 'normal5', 'he5'],\n",
    "                                  title='',\n",
    "                                  ylabel=''):\n",
    "    '''Plots either the training loss history, val loss history, or val acc history for the 6 networks.\n",
    "\n",
    "    This plotting function is provided to you. Feel free to modify to suit your needs.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    hists: list of lists. len(hists)=6. len(sublists) = E, where E is number of epochs used to train.\n",
    "        The training loss, val loss, OR val acc history for the 6 networks.\n",
    "    labels: list of str.\n",
    "        The labels/order of conditions in the hists. Modify if your order does not match the default.\n",
    "    title: str.\n",
    "        A useful title.\n",
    "    ylabel: str.\n",
    "        A useful y label.\n",
    "    '''\n",
    "    if len(hists) != len(labels):\n",
    "        print('The length of the hists needs to match the labels length â€” one label per history')\n",
    "        return\n",
    "\n",
    "    rng = np.random.default_rng(0)\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(8, 4))\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        hist = hists[i]\n",
    "        label = labels[i]\n",
    "        shift = rng.uniform(low=-0.1, high=0.1, size=(1,))\n",
    "        if 'normal' in label:\n",
    "            axes[0].plot(hist+shift, label=label)\n",
    "        else:\n",
    "            axes[1].plot(hist+shift, label=label)\n",
    "\n",
    "    axes[0].legend()\n",
    "    axes[1].legend()\n",
    "    fig.suptitle(title)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel(ylabel)\n",
    "    plt.show()\n",
    "\n",
    "# This code is here to help you plot your results and test accuracies. It will almost certainly not exactly work\n",
    "# with your variables. Feel free to adapt to suit your needs or delete and use your own code.\n",
    "plot_loss_and_acc_wt_init_exp(list(training_losses.values()),\n",
    "                              title='VGG6 CIFAR-10 training losses', ylabel='Loss')\n",
    "plot_loss_and_acc_wt_init_exp(list(val_losses.values()),\n",
    "                              title='VGG6 CIFAR-10 val losses', ylabel='Loss')\n",
    "plot_loss_and_acc_wt_init_exp(list(val_accs.values()),\n",
    "                              title='VGG6 CIFAR-10 val acc', ylabel='Acc')\n",
    "\n",
    "print('Test accs:')\n",
    "for label, acc in test_accs.items():\n",
    "    print(f'{label}: {100*acc:.2f} in {epochs[label]} epochs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6e. Questions\n",
    "\n",
    "**Question 5:** How did the networks initialized with the Normal method compare to those initialized with He?\n",
    "\n",
    "**Question 6:** How does the test accuracy achieved by the best performing VGG6 network compare to:\n",
    "- the previous best VGG6 result you obtained (before this Task)?\n",
    "- the best VGG4 result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 5:**\n",
    "\n",
    "**Answer 6:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Going deeper with VGG8\n",
    "\n",
    "With deeper networks, overfitting becomes a major concern. In this task, we will explore two ways to regularize deep neural networks and analyze the impact they have on classification accuracy.\n",
    "\n",
    "### 7a. Implement VGG8\n",
    "\n",
    "In `vgg_nets.py` implement the `VGG8` class. The VGG8 network has the following architecture:\n",
    "\n",
    "Conv2D â†’ Conv2D â†’ MaxPool2D â†’ Conv2D â†’ Conv2D â†’ MaxPool2D â†’ Conv2D â†’ Conv2D â†’ MaxPool2D â†’ Flatten â†’ Dense â†’ Dropout â†’ Dense\n",
    "\n",
    "If you copy-paste from VGG6, this should be fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vgg_nets import VGG8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `VGG8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = VGG8(C=5, input_feats_shape=(24, 24, 10), wt_init='he')\n",
    "v.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above should print:\n",
    "\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "Dense layer output(output) shape: [1, 5]\n",
    "DenseBlock1:\n",
    "\tDropout layer output(DenseBlock1/dropout) shape: [1, 512]\n",
    "\tDense layer output(DenseBlock1/dense0) shape: [1, 512]\n",
    "Flatten layer output(flat) shape: [1, 2304]\n",
    "ConvBlock3:\n",
    "\tMaxPool2D layer output(ConvBlock3/maxpool2) shape: [1, 3, 3, 256]\n",
    "\tConv2D layer output(ConvBlock3/conv1) shape: [1, 6, 6, 256]\n",
    "\tConv2D layer output(ConvBlock3/conv0) shape: [1, 6, 6, 256]\n",
    "ConvBlock2:\n",
    "\tMaxPool2D layer output(ConvBlock2/maxpool2) shape: [1, 6, 6, 128]\n",
    "\tConv2D layer output(ConvBlock2/conv1) shape: [1, 12, 12, 128]\n",
    "\tConv2D layer output(ConvBlock2/conv0) shape: [1, 12, 12, 128]\n",
    "ConvBlock1:\n",
    "\tMaxPool2D layer output(ConvBlock1/maxpool2) shape: [1, 12, 12, 64]\n",
    "\tConv2D layer output(ConvBlock1/conv1) shape: [1, 24, 24, 64]\n",
    "\tConv2D layer output(ConvBlock1/conv0) shape: [1, 24, 24, 64]\n",
    "---------------------------------------------------------------------------\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "v = VGG8(C=5, input_feats_shape=(12, 12, 10), wt_init='he')\n",
    "tf.random.set_seed(1)\n",
    "net_acts = v(tf.random.uniform([1, 12, 12, 10]))\n",
    "\n",
    "print(f'Your net_acts are\\n{net_acts.numpy()} and should be')\n",
    "print('''[[0.132 0.207 0.576 0.025 0.06 ]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7b. Effects of dropout\n",
    "\n",
    "In the cell below, train VGG8 with and without dropout in the convolutional blocks (i.e. train 2 separate nets). Train with a patience of `4` and use He initialization.\n",
    "\n",
    "After training each net, store/record:\n",
    "1. the network's final test accuracy.\n",
    "2. the history of training loss (averaged across mini-batches, so one per epoch).\n",
    "3. the history of validation loss (checked every epoch).\n",
    "4. the history of validation accuracy (checked every epoch).\n",
    "\n",
    "**Notes:**\n",
    "- Set random seed before creating each net for consistency.\n",
    "- Use `tf.keras.backend.clear_session()` before creating each of the network to help prevent the memory used from growing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function below to plot:\n",
    "1. The training loss history.\n",
    "2. The val loss history.\n",
    "3. The val acc history.\n",
    "\n",
    "Afterwards print the test accuracy for each network along wth the number of actual training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_dropout_exp(hist_nodropout, hist_dropout, title, ylabel):\n",
    "    '''Plots either the training loss history, val loss history, or val acc history for the net trained with and\n",
    "    without dropout.\n",
    "\n",
    "    This plotting function is provided to you. Feel free to modify to suit your needs.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    hist_nodropout: array-like. len(hist_nodropout)=E, where E is number of epochs used to train.\n",
    "        The training loss, val loss, OR val acc history for the net trained without dropout.\n",
    "    hist_dropout: array-like. len(hist_dropout)=E, where E is number of epochs used to train.\n",
    "        The training loss, val loss, OR val acc history for the net trained with dropout.\n",
    "    title: str.\n",
    "        A useful title.\n",
    "    ylabel: str.\n",
    "        A useful y label.\n",
    "    '''\n",
    "    plt.plot(hist_nodropout, label='No dropout')\n",
    "    plt.plot(hist_dropout, label='Dropout')\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7c. Questions\n",
    "\n",
    "**Question 7:** What effect does the dropout have on the training and validation loss? What accounts the difference you observe? Please be specific.\n",
    "\n",
    "**Question 8:** What effect does the dropout have on the CIFAR10 test accuracy you achieved?\n",
    "\n",
    "**Question 9:** How does the test accuracy achieved with VGG8 (with and without dropout) compare to the best performing VGG6 network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 7:**\n",
    "\n",
    "**Answer 8:**\n",
    "\n",
    "**Answer 9:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7d. Add support for AdamW optimizer\n",
    "\n",
    "Another way to combat overfitting is more \"traditional\" regularization. But Adam may no longer be best choice when introducing this into training.\n",
    "\n",
    "In the `compile` method of `DeepNetwork`, add support for the AdamW optimizer (suggested string to select: `'adamw'`). When you create the AdamW optimizer object, specify the learning rate AND set the `weight_decay` keyword argument to the network's regularization strength. Since we are using the Keras built-in optimizer, this should be a one-liner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7e. Regularization and choice of optimizer\n",
    "\n",
    "In the cell below, train VGG8 repeatedly with the following combinations of optimizer and regularization choices (*4 nets trained total*):\n",
    "\n",
    "**(Reg strength, optimizer):** `(0.6, Adam), (0.6, AdamW), (0.06, Adam), (0.0, Adam)`\n",
    "\n",
    "Do **not** use dropout in the conv blocks. Train with a patience of `4`. Use He initialization.\n",
    "\n",
    "After training each net, store/record:\n",
    "1. the network's final test accuracy.\n",
    "2. the history of training loss (averaged across mini-batches, so one per epoch).\n",
    "3. the history of validation loss (checked every epoch).\n",
    "4. the history of validation accuracy (checked every epoch).\n",
    "\n",
    "**Notes:**\n",
    "- Set random seed before creating each net for consistency.\n",
    "- Use `tf.keras.backend.clear_session()` before creating each of the network to help prevent the memory used from growing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_optimizer_exp(adamw_hist, adam_hists, adam_labels=['adam_reg', 'adam_low_reg', 'adam_no_reg'],\n",
    "                            title='',\n",
    "                            ylabel=''):\n",
    "    '''Plots either the training loss history, val loss history, or val acc history for the nets trained with adam and\n",
    "    adamw.\n",
    "\n",
    "    This plotting function is provided to you. Feel free to modify to suit your needs.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    adamw_hist: array-like. len(hist_nodropout)=E, where E is number of epochs used to train.\n",
    "        The training loss, val loss, OR val acc history for the net trained with Adam.\n",
    "    adam_hists: list of lists. len(hists)=3. len(sublists) = E, where E is number of epochs used to train.\n",
    "        The training loss, val loss, OR val acc history for the 3 networks.\n",
    "    labels: list of str.\n",
    "        The labels/order of conditions in adam_hists. Modify if your order does not match the default.\n",
    "    title: str.\n",
    "        A useful title.\n",
    "    ylabel: str.\n",
    "        A useful y label.\n",
    "    '''\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(8,4))\n",
    "\n",
    "    axes[0].plot(adamw_hist, label='adamw_reg')\n",
    "    for i in range(len(adam_labels)):\n",
    "        axes[1].plot(adam_hists[i], label=adam_labels[i])\n",
    "\n",
    "    axes[0].legend()\n",
    "    axes[1].legend()\n",
    "    fig.suptitle(title)\n",
    "    axes[0].set_ylabel(ylabel)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    plt.show()\n",
    "\n",
    "# This code is here to help you plot your results and test accuracies. It will almost certainly not exactly work\n",
    "# with your variables. Feel free to adapt to suit your needs or delete and use your own code.\n",
    "plot_hist_optimizer_exp(train_losses[0], train_losses[1:], title='VGG8 CIFAR-10 training losses', ylabel='Loss')\n",
    "plot_hist_optimizer_exp(val_losses[0], val_losses[1:], title='VGG8 CIFAR-10 val losses', ylabel='Loss')\n",
    "plot_hist_optimizer_exp(val_accs[0], val_accs[1:], title='VGG8 CIFAR-10 val accs', ylabel='Accuracy')\n",
    "\n",
    "\n",
    "all_labels = ['adamw_reg', 'adam_reg', 'adam_low_reg', 'adam_no_reg']\n",
    "print('Test accs:')\n",
    "print()\n",
    "for i in range(len(all_labels)):\n",
    "    print(f'{all_labels[i]}: {100*test_accs[i]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7c. Questions\n",
    "\n",
    "**Question 10:** What effect does regularization have with Adam? When does it perform best/worst? How do you know?\n",
    "\n",
    "**Question 11:** How does the results with AdamW compare with the nets trained with Adam?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 10:**\n",
    "\n",
    "**Answer 11:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Going even deeper with VGG15\n",
    "\n",
    "This network is an approximation of the VGG19 network from the VGG paper. This is a big and deep model:\n",
    "- 5 conv blocks. First two blocks have 2 conv layers, each. The remaining blocks have 3 conv layers each. There is no dropout in any of the conv blocks.\n",
    "- flatten layer.\n",
    "- 1 dense block with dropout.\n",
    "- 1 dense output layer with softmax activation.\n",
    "\n",
    "In this task, the goal is to implement VGG15 and assess its baseline accuracy on CIFAR-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8a. Implement and test VGG15\n",
    "\n",
    "If you copy-paste from VGG8, this should be fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vgg_nets import VGG15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `VGG15`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = VGG15(C=5, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "v.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above should print:\n",
    "\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "Dense layer output(output) shape: [1, 5]\n",
    "DenseBlock1:\n",
    "\tDropout layer output(DenseBlock1/dropout) shape: [1, 512]\n",
    "\tDense layer output(DenseBlock1/dense0) shape: [1, 512]\n",
    "Flatten layer output(flat) shape: [1, 512]\n",
    "ConvBlock5:\n",
    "\tMaxPool2D layer output(ConvBlock5/maxpool2) shape: [1, 1, 1, 512]\n",
    "\tConv2D layer output(ConvBlock5/conv2) shape: [1, 2, 2, 512]\n",
    "\tConv2D layer output(ConvBlock5/conv1) shape: [1, 2, 2, 512]\n",
    "\tConv2D layer output(ConvBlock5/conv0) shape: [1, 2, 2, 512]\n",
    "ConvBlock4:\n",
    "\tMaxPool2D layer output(ConvBlock4/maxpool2) shape: [1, 2, 2, 512]\n",
    "\tConv2D layer output(ConvBlock4/conv2) shape: [1, 4, 4, 512]\n",
    "\tConv2D layer output(ConvBlock4/conv1) shape: [1, 4, 4, 512]\n",
    "\tConv2D layer output(ConvBlock4/conv0) shape: [1, 4, 4, 512]\n",
    "ConvBlock3:\n",
    "\tMaxPool2D layer output(ConvBlock3/maxpool2) shape: [1, 4, 4, 256]\n",
    "\tConv2D layer output(ConvBlock3/conv2) shape: [1, 8, 8, 256]\n",
    "\tConv2D layer output(ConvBlock3/conv1) shape: [1, 8, 8, 256]\n",
    "\tConv2D layer output(ConvBlock3/conv0) shape: [1, 8, 8, 256]\n",
    "ConvBlock2:\n",
    "\tMaxPool2D layer output(ConvBlock2/maxpool2) shape: [1, 8, 8, 128]\n",
    "\tConv2D layer output(ConvBlock2/conv1) shape: [1, 16, 16, 128]\n",
    "\tConv2D layer output(ConvBlock2/conv0) shape: [1, 16, 16, 128]\n",
    "ConvBlock1:\n",
    "\tMaxPool2D layer output(ConvBlock1/maxpool2) shape: [1, 16, 16, 64]\n",
    "\tConv2D layer output(ConvBlock1/conv1) shape: [1, 32, 32, 64]\n",
    "\tConv2D layer output(ConvBlock1/conv0) shape: [1, 32, 32, 64]\n",
    "---------------------------------------------------------------------------\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "v = VGG15(C=5, input_feats_shape=(32, 32, 3), wt_init='he')\n",
    "tf.random.set_seed(1)\n",
    "net_acts = v(tf.random.uniform([3, 32, 32, 3]))\n",
    "\n",
    "print(f'Your net_acts are\\n{net_acts.numpy()} and should be')\n",
    "print('''[[0.096 0.298 0.203 0.319 0.084]\n",
    " [0.089 0.278 0.216 0.33  0.087]\n",
    " [0.087 0.277 0.214 0.345 0.078]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8b. Train VGG15 on CIFAR-10\n",
    "\n",
    "Here is how you should setup training:\n",
    "1. Train with patience of `4`.\n",
    "2. Use He initialization.\n",
    "3. Use AdamW and regularization of `0.6`.\n",
    "\n",
    "Store/record:\n",
    "1. the network's final test accuracy.\n",
    "2. the history of training loss (averaged across mini-batches, so one per epoch).\n",
    "3. the history of validation loss (checked every epoch).\n",
    "\n",
    "**Notes:**\n",
    "- Set random seed at the beginning for consistency.\n",
    "- Use `tf.keras.backend.clear_session()` at the beginning to help prevent the memory used from growing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "tf.keras.backend.clear_session()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, plot the VGG15 train and validation loss history over epochs. Place the test acc in the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8c. Questions\n",
    "\n",
    "**Question 12:** How does the baseline accuracy of VGG15 compare to the best accuracies you achieved with the shallower networks? Did this surprise you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 12:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs444_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
