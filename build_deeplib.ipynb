{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR NAMES HERE**\n",
    "\n",
    "Spring 2025\n",
    "\n",
    "CS 444: Deep Learning\n",
    "\n",
    "Project 1: Deep Neural Networks \n",
    "\n",
    "#### Week 1: VGG4 and building a deep learning library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement familiar neural network layers: Dense, Dropout, Flatten, MaxPool2D, Conv2D\n",
    "\n",
    "Now that the CIFAR10 and MNIST data loading and preprocessing pipeline is ready, implement and test the layers that will be assembled to create a deep VGG neural network.\n",
    "\n",
    "There are many methods to implement here, but they are mostly small — either one or just several lines of code. *You implemented all these methods in CS343 in NumPy so it should be helpful to have your CS343 CNN project open. The new element here is writing them in TensorFlow rather than NumPy.*\n",
    "\n",
    "**NOTE:**\n",
    "- For reasons that will become clear very quickly, it is **critical** to have your code run fast and on the GPU. This means **you must write 100% TensorFlow code** in `layers.py`. If you use any NumPy your neural network may not work or run too slowly to do anything useful!\n",
    "- Ignore the methods that have (Week 2) or (Week 3) in the docstrings for now. You will implement those in a few weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Implement and test `Layer` class methods\n",
    "\n",
    "These methods are in the `Layer` class in `layers.py`:\n",
    "- Constructor\n",
    "- Get/set methods: `get_name`, `get_act_fun_name`, `get_prev_layer_or_block`, `get_wts`, `get_b`, `has_wts`, `get_mode`, `set_mode`\n",
    "- `compute_net_activation(net_in)`\n",
    "- `__call__(x)`. Does the forward pass thru the layer. Uses the functional API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: Basic `Layer` methods\n",
    "\n",
    "**NOTE:** You should never instantiate `Layer` objects, but we do so here only to test your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The layer is called Dense_0 and has linear activation.\n",
      "You should see:\n",
      "The layer is called Dense_0 and has linear activation.\n",
      "The previous layer is None\n",
      "The previous layer is None and should be None.\n",
      "The layer is training?\n",
      "<tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False>. You should see\n",
      "<tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False>\n",
      "Does the layer have wts? False. It should be False.\n",
      "Setting the network to training mode.\n",
      "The layer is training?\n",
      "<tf.Variable 'Variable:0' shape=() dtype=bool, numpy=True>. You should see\n",
      "<tf.Variable 'Variable:0' shape=() dtype=bool, numpy=True>\n"
     ]
    }
   ],
   "source": [
    "# Test hidden_0\n",
    "hidden_0 = Layer('Dense_0', activation='linear', prev_layer_or_block=None)\n",
    "print(f'The layer is called {hidden_0.get_name()} and has {hidden_0.get_act_fun_name()} activation.')\n",
    "print('You should see:')\n",
    "print('The layer is called Dense_0 and has linear activation.\\nThe previous layer is None')\n",
    "print(f'The previous layer is {hidden_0.get_prev_layer_or_block()} and should be None.')\n",
    "print(f'The layer is training?\\n{hidden_0.get_mode()}. You should see')\n",
    "print(\"<tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False>\")\n",
    "print(f'Does the layer have wts? {hidden_0.has_wts()}. It should be False.')\n",
    "print('Setting the network to training mode.')\n",
    "hidden_0.set_mode(True)\n",
    "print(f'The layer is training?\\n{hidden_0.get_mode()}. You should see')\n",
    "print(\"<tf.Variable 'Variable:0' shape=() dtype=bool, numpy=True>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The layer is called Dense_1 and has relu activation.\n",
      "You should see:\n",
      "The layer is called Dense_1 and has relu activation.\n",
      "The previous layer is None\n",
      "The previous layer is Dense_0 and should be Dense_0.\n"
     ]
    }
   ],
   "source": [
    "# Test hidden_1\n",
    "hidden_1 = Layer('Dense_1', activation='relu', prev_layer_or_block=hidden_0)\n",
    "print(f'The layer is called {hidden_1.get_name()} and has {hidden_1.get_act_fun_name()} activation.')\n",
    "print('You should see:')\n",
    "print('The layer is called Dense_1 and has relu activation.\\nThe previous layer is None')\n",
    "print(f'The previous layer is {hidden_1.get_prev_layer_or_block().get_name()} and should be Dense_0.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: computing net activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your activations from Dense_0:\n",
      "[[-0.8321 -1.1737  0.1416]\n",
      " [ 0.245  -0.3333  1.2313]]\n",
      "they should be:\n",
      "[[-0.8321 -1.1737  0.1416]\n",
      " [ 0.245  -0.3333  1.2313]]\n",
      "Your activations from Dense_1:\n",
      "[[0.     0.     0.1416]\n",
      " [0.245  0.     1.2313]]\n",
      "they should be:\n",
      "[[0.     0.     0.1416]\n",
      " [0.245  0.     1.2313]]\n",
      "Your activations from Dense_1 (after changing act fun to softmax):\n",
      "[[0.2295 0.163  0.6075]\n",
      " [0.2357 0.1322 0.6321]]\n",
      "they should be:\n",
      "[[0.2295 0.163  0.6075]\n",
      " [0.2357 0.1322 0.6321]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "x_test_input = tf.random.uniform(shape=(2, 3), minval=-2, maxval=2)\n",
    "print('Your activations from Dense_0:')\n",
    "test_acts = hidden_0.compute_net_activation(x_test_input)\n",
    "print(test_acts.numpy())\n",
    "print('they should be:')\n",
    "print('''[[-0.8321 -1.1737  0.1416]\n",
    " [ 0.245  -0.3333  1.2313]]''')\n",
    "print('Your activations from Dense_1:')\n",
    "test_acts = hidden_1.compute_net_activation(x_test_input)\n",
    "print(test_acts.numpy())\n",
    "print('they should be:')\n",
    "print('''[[0.     0.     0.1416]\n",
    " [0.245  0.     1.2313]]''')\n",
    "print('Your activations from Dense_1 (after changing act fun to softmax):')\n",
    "hidden_1.act_fun_name = 'softmax'\n",
    "test_acts = hidden_1.compute_net_activation(x_test_input)\n",
    "print(test_acts.numpy())\n",
    "print('they should be:')\n",
    "print('''[[0.2295 0.163  0.6075]\n",
    " [0.2357 0.1322 0.6321]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Implement and test `Dense` layer class methods\n",
    "\n",
    "These methods are in the `Dense` class in `layers.py`:\n",
    "- Constructor\n",
    "- `has_wts`\n",
    "- `init_params(input_shape)`\n",
    "- `compute_net_input(x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: Dense layer basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense layer is called Dense_2 and has previous layer Layer_0\n",
      "You should see:\n",
      "Dense layer is called Dense_2 and has previous layer Layer_0\n",
      "Does the layer have weights? True. It should be True.\n",
      "----------Initializing wts and biases Test1/2----------\n",
      "Your wts:\n",
      "[[-0.1101  0.1546  0.0384 -0.088  -0.1225 -0.0981  0.0088]\n",
      " [-0.0203 -0.0558 -0.0721 -0.0626 -0.0715 -0.0348 -0.0336]\n",
      " [ 0.0183  0.1109  0.128  -0.0021 -0.032   0.0373  0.0253]] They should be:\n",
      "[[-0.1101  0.1546  0.0384 -0.088  -0.1225 -0.0981  0.0088]\n",
      " [-0.0203 -0.0558 -0.0721 -0.0626 -0.0715 -0.0348 -0.0336]\n",
      " [ 0.0183  0.1109  0.128  -0.0021 -0.032   0.0373  0.0253]]\n",
      "Your biases:\n",
      "[ 0.0403 -0.1088 -0.0063  0.1337  0.0712 -0.0489 -0.0764] They should be:\n",
      "[ 0.0403 -0.1088 -0.0063  0.1337  0.0712 -0.0489 -0.0764]\n",
      "----------Initializing wts and biases Test2/2----------\n",
      "Your wts:\n",
      "[[-0.0457 -0.0407  0.0729 -0.0893  0.0313  0.0994 -0.1784]\n",
      " [-0.0522  0.0981 -0.0676  0.1146  0.0206 -0.0197  0.0538]\n",
      " [ 0.0764 -0.0836  0.0336  0.156   0.0723  0.1075 -0.0266]] They should be:\n",
      "[[-0.0457 -0.0407  0.0729 -0.0893  0.0313  0.0994 -0.1784]\n",
      " [-0.0522  0.0981 -0.0676  0.1146  0.0206 -0.0197  0.0538]\n",
      " [ 0.0764 -0.0836  0.0336  0.156   0.0723  0.1075 -0.0266]]\n",
      "Your biases:\n",
      "[ 0.1694  0.012  -0.1158  0.0173 -0.0714  0.069  -0.1091] They should be:\n",
      "[ 0.1694  0.012  -0.1158  0.0173 -0.0714  0.069  -0.1091]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "# x_test_input2 = tf.random.uniform(shape=(2, 7), minval=-2, maxval=2)\n",
    "layer_0 = Layer('Layer_0', activation='linear', prev_layer_or_block=None)\n",
    "hidden_2 = Dense('Dense_2', 7, activation='linear', prev_layer_or_block=layer_0, wt_scale=1e-1)\n",
    "print(f'Dense layer is called {hidden_2.get_name()} and has previous layer {hidden_2.get_prev_layer_or_block().get_name()}')\n",
    "print('You should see:')\n",
    "print('Dense layer is called Dense_2 and has previous layer Layer_0')\n",
    "print(f'Does the layer have weights? {hidden_2.has_wts()}. It should be True.')\n",
    "\n",
    "print('----------Initializing wts and biases Test1/2----------')\n",
    "hidden_2.init_params(input_shape=(2, 3))\n",
    "print(f'Your wts:\\n{hidden_2.get_wts().numpy()} They should be:')\n",
    "print('''[[-0.1101  0.1546  0.0384 -0.088  -0.1225 -0.0981  0.0088]\n",
    " [-0.0203 -0.0558 -0.0721 -0.0626 -0.0715 -0.0348 -0.0336]\n",
    " [ 0.0183  0.1109  0.128  -0.0021 -0.032   0.0373  0.0253]]''')\n",
    "print(f'Your biases:\\n{hidden_2.get_b().numpy()} They should be:')\n",
    "print('''[ 0.0403 -0.1088 -0.0063  0.1337  0.0712 -0.0489 -0.0764]''')\n",
    "print('----------Initializing wts and biases Test2/2----------')\n",
    "hidden_2.init_params(input_shape=(4, 5, 2, 3))\n",
    "print(f'Your wts:\\n{hidden_2.get_wts().numpy()} They should be:')\n",
    "print('''[[-0.0457 -0.0407  0.0729 -0.0893  0.0313  0.0994 -0.1784]\n",
    " [-0.0522  0.0981 -0.0676  0.1146  0.0206 -0.0197  0.0538]\n",
    " [ 0.0764 -0.0836  0.0336  0.156   0.0723  0.1075 -0.0266]]''')\n",
    "print(f'Your biases:\\n{hidden_2.get_b().numpy()} They should be:')\n",
    "print('''[ 0.1694  0.012  -0.1158  0.0173 -0.0714  0.069  -0.1091]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: Dense layer forward pass 1/2\n",
    "\n",
    "This test calls the rest of your `Dense` layer methods via your`__call__` implementation in `Layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your activations from Dense_2 w/ linear:\n",
      "[[ 0.1868  0.1802 -0.3044  0.4026 -0.0424 -0.0396  0.2024]\n",
      " [ 0.2684 -0.1067 -0.0595  0.0342 -0.0556  0.1206 -0.1223]\n",
      " [ 0.1449 -0.1445  0.0619 -0.0825  0.029   0.3322 -0.4797]\n",
      " [ 0.1567  0.0102 -0.1068  0.1138 -0.014   0.1491 -0.1667]]\n",
      "they should be:\n",
      "[[ 0.1868  0.1802 -0.3044  0.4026 -0.0424 -0.0396  0.2024]\n",
      " [ 0.2684 -0.1067 -0.0595  0.0342 -0.0556  0.1206 -0.1223]\n",
      " [ 0.1449 -0.1445  0.0619 -0.0825  0.029   0.3322 -0.4797]\n",
      " [ 0.1567  0.0102 -0.1068  0.1138 -0.014   0.1491 -0.1667]]\n",
      "Your activations from Dense_2 w/ relu:\n",
      "[[0.1868 0.1802 0.     0.4026 0.     0.     0.2024]\n",
      " [0.2684 0.     0.     0.0342 0.     0.1206 0.    ]\n",
      " [0.1449 0.     0.0619 0.     0.029  0.3322 0.    ]\n",
      " [0.1567 0.0102 0.     0.1138 0.     0.1491 0.    ]]\n",
      "they should be:\n",
      "[[0.1868 0.1802 0.     0.4026 0.     0.     0.2024]\n",
      " [0.2684 0.     0.     0.0342 0.     0.1206 0.    ]\n",
      " [0.1449 0.     0.0619 0.     0.029  0.3322 0.    ]\n",
      " [0.1567 0.0102 0.     0.1138 0.     0.1491 0.    ]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "x_test_input2 = tf.random.uniform(shape=(4, 3), minval=-2, maxval=2)\n",
    "print('Your activations from Dense_2 w/ linear:')\n",
    "hidden_2.act_fun_name = 'linear'\n",
    "test_acts = hidden_2(x_test_input2)\n",
    "print(test_acts.numpy())\n",
    "print('they should be:')\n",
    "print('''[[ 0.1868  0.1802 -0.3044  0.4026 -0.0424 -0.0396  0.2024]\n",
    " [ 0.2684 -0.1067 -0.0595  0.0342 -0.0556  0.1206 -0.1223]\n",
    " [ 0.1449 -0.1445  0.0619 -0.0825  0.029   0.3322 -0.4797]\n",
    " [ 0.1567  0.0102 -0.1068  0.1138 -0.014   0.1491 -0.1667]]''')\n",
    "print('Your activations from Dense_2 w/ relu:')\n",
    "hidden_2.act_fun_name = 'relu'\n",
    "test_acts = hidden_2(x_test_input2)\n",
    "print(test_acts.numpy())\n",
    "print('they should be:')\n",
    "print('''[[0.1868 0.1802 0.     0.4026 0.     0.     0.2024]\n",
    " [0.2684 0.     0.     0.0342 0.     0.1206 0.    ]\n",
    " [0.1449 0.     0.0619 0.     0.029  0.3322 0.    ]\n",
    " [0.1567 0.0102 0.     0.1138 0.     0.1491 0.    ]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: Dense layer forward pass 2/2\n",
    "\n",
    "This tests lazy initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your activations from Dense_2 w/ linear:\n",
      "[[0.1868 0.1802 0.     0.4026 0.     0.     0.2024]\n",
      " [0.2684 0.     0.     0.0342 0.     0.1206 0.    ]\n",
      " [0.1449 0.     0.0619 0.     0.029  0.3322 0.    ]\n",
      " [0.1567 0.0102 0.     0.1138 0.     0.1491 0.    ]]\n",
      "they should be:\n",
      "[[0.1868 0.1802 0.     0.4026 0.     0.     0.2024]\n",
      " [0.2684 0.     0.     0.0342 0.     0.1206 0.    ]\n",
      " [0.1449 0.     0.0619 0.     0.029  0.3322 0.    ]\n",
      " [0.1567 0.0102 0.     0.1138 0.     0.1491 0.    ]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "x_test_input2 = tf.random.uniform(shape=(4, 3), minval=-2, maxval=2)\n",
    "print('Your activations from Dense_2 w/ linear:')\n",
    "for i in range(5):\n",
    "    test_acts = hidden_2(x_test_input2)\n",
    "print(test_acts.numpy())\n",
    "print('they should be:')\n",
    "print('''[[0.1868 0.1802 0.     0.4026 0.     0.     0.2024]\n",
    " [0.2684 0.     0.     0.0342 0.     0.1206 0.    ]\n",
    " [0.1449 0.     0.0619 0.     0.029  0.3322 0.    ]\n",
    " [0.1567 0.0102 0.     0.1138 0.     0.1491 0.    ]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `__str__` and output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense layer output(Dense_2) shape: [4, 7]\n",
      "The above should print:\n",
      "Dense layer output(Dense_2) shape: [4, 7]\n"
     ]
    }
   ],
   "source": [
    "print(hidden_2)\n",
    "print('The above should print:')\n",
    "print('Dense layer output(Dense_2) shape: [4, 7]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Implement and test `Dropout` layer class methods\n",
    "\n",
    "Please complete the `Dropout` class then test your implementation below.\n",
    "\n",
    "**Note:** If you did not learn dropout layers in CS343, please see the supplementary video on the notes website. Please come talk to me if any questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `Dropout` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Test1/2-------------------------\n",
      "The layer preceding your dropout layer is Blarg. It should be Blarg.\n",
      "Your Dropout net_in is:\n",
      "[[ 0.      1.4097 -0.     -0.     -0.    ]\n",
      " [ 0.     -0.0466  0.      0.      0.    ]\n",
      " [-0.     -0.      2.6454 -0.     -3.1994]\n",
      " [-0.     -1.2027 -0.      1.0128  1.7384]]\n",
      "It should be EITHER:\n",
      "[[ 0.      1.4097 -0.     -0.     -0.    ]\n",
      " [ 0.     -0.0466  0.      0.      0.    ]\n",
      " [-0.     -0.      2.6454 -0.     -3.1994]\n",
      " [-0.     -1.2027 -0.      1.0128  1.7384]]\n",
      "or:\n",
      "[[ 0.      0.     -1.399  -3.4535 -0.    ]\n",
      " [ 0.     -0.      3.9629  0.      0.    ]\n",
      " [-2.3524 -0.      0.     -2.325  -0.    ]\n",
      " [-3.0023 -0.     -0.      0.      0.    ]]\n",
      "-------------------------Test2/2-------------------------\n",
      "Your Dropout net_act is:\n",
      "[[ 1.5111  0.4229 -0.4197 -1.036  -1.2368]\n",
      " [ 0.4703 -0.014   1.1889  0.6025  0.5997]\n",
      " [-0.7057 -0.433   0.7936 -0.6975 -0.9598]\n",
      " [-0.9007 -0.3608 -0.2238  0.3038  0.5215]]\n",
      "It should be:\n",
      "[[ 1.5111  0.4229 -0.4197 -1.036  -1.2368]\n",
      " [ 0.4703 -0.014   1.1889  0.6025  0.5997]\n",
      " [-0.7057 -0.433   0.7936 -0.6975 -0.9598]\n",
      " [-0.9007 -0.3608 -0.2238  0.3038  0.5215]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "print('-------------------------Test1/2-------------------------')\n",
    "x_test_input_drop = tf.random.normal(shape=(4, 5))\n",
    "dummy_layer = Layer('Blarg', activation='linear', prev_layer_or_block=None)\n",
    "drop = Dropout('Dropout_Layer', rate=0.7, prev_layer_or_block=dummy_layer)\n",
    "drop.set_mode(True)\n",
    "drop_net_in = drop.compute_net_input(x_test_input_drop)\n",
    "print(f'The layer preceding your dropout layer is {drop.get_prev_layer_or_block().get_name()}. It should be Blarg.')\n",
    "print(f'Your Dropout net_in is:\\n{drop_net_in}')\n",
    "print('It should be EITHER:')\n",
    "print('''[[ 0.      1.4097 -0.     -0.     -0.    ]\n",
    " [ 0.     -0.0466  0.      0.      0.    ]\n",
    " [-0.     -0.      2.6454 -0.     -3.1994]\n",
    " [-0.     -1.2027 -0.      1.0128  1.7384]]''')\n",
    "print('or:')\n",
    "print('''[[ 0.      0.     -1.399  -3.4535 -0.    ]\n",
    " [ 0.     -0.      3.9629  0.      0.    ]\n",
    " [-2.3524 -0.      0.     -2.325  -0.    ]\n",
    " [-3.0023 -0.     -0.      0.      0.    ]]''')\n",
    "print('-------------------------Test2/2-------------------------')\n",
    "drop.set_mode(False)\n",
    "drop_net_act = drop(x_test_input_drop)\n",
    "print(f'Your Dropout net_act is:\\n{drop_net_act}')\n",
    "print('It should be:')\n",
    "print('''[[ 1.5111  0.4229 -0.4197 -1.036  -1.2368]\n",
    " [ 0.4703 -0.014   1.1889  0.6025  0.5997]\n",
    " [-0.7057 -0.433   0.7936 -0.6975 -0.9598]\n",
    " [-0.9007 -0.3608 -0.2238  0.3038  0.5215]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `Dropout` layer `__str__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout layer output(Dropout_Layer) shape: [4, 5]\n",
      "The above should print:\n",
      "Dropout layer output(Dropout_Layer) shape: [4, 5]\n"
     ]
    }
   ],
   "source": [
    "print(drop)\n",
    "print('The above should print:')\n",
    "print('Dropout layer output(Dropout_Layer) shape: [4, 5]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d. Implement and test `Flatten` layer class methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `Flatten` forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Flatten layer net_act is:\n",
      "[[ 1.5111  0.4229 -0.4197 -1.036  -1.2368  0.4703]\n",
      " [-0.014   1.1889  0.6025  0.5997 -0.7057 -0.433 ]]\n",
      "It should be:\n",
      "[[ 1.5111  0.4229 -0.4197 -1.036  -1.2368  0.4703]\n",
      " [-0.014   1.1889  0.6025  0.5997 -0.7057 -0.433 ]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "x_test_input_flat = tf.random.normal(shape=(2, 1, 2, 3))\n",
    "flat = Flatten('pancake', prev_layer_or_block=None)\n",
    "flat_acts = flat(x_test_input_flat)\n",
    "print(f'Your Flatten layer net_act is:\\n{flat_acts}')\n",
    "print('It should be:')\n",
    "print('''[[ 1.5111  0.4229 -0.4197 -1.036  -1.2368  0.4703]\n",
    " [-0.014   1.1889  0.6025  0.5997 -0.7057 -0.433 ]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `Flatten` layer `__str__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flatten layer output(pancake) shape: [2, 6]\n",
      "The above should print:\n",
      "Flatten layer output(pancake) shape: [2, 6]\n"
     ]
    }
   ],
   "source": [
    "print(flat)\n",
    "print('The above should print:')\n",
    "print('Flatten layer output(pancake) shape: [2, 6]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d. Implement and test `MaxPool2D` layer class methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import MaxPool2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `MaxPool2D` forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1/2...\n",
      "Layer has weights? False. Should be False.\n",
      "Your MaxPool2D layer net_act is:\n",
      "[[[[1.5111 0.4229 0.4703]\n",
      "   [0.7936 1.0278 0.3279]]\n",
      "\n",
      "  [[2.0615 0.1364 1.2597]\n",
      "   [0.6657 1.2569 0.4754]]]\n",
      "\n",
      "\n",
      " [[[0.711  0.4818 0.8706]\n",
      "   [0.1162 0.268  0.8781]]\n",
      "\n",
      "  [[1.6651 1.3796 1.2738]\n",
      "   [2.2517 1.7184 0.3346]]]]\n",
      "It should be:\n",
      "[[[[1.5111 0.4229 0.4703]\n",
      "   [0.7936 1.0278 0.3279]]\n",
      "\n",
      "  [[2.0615 0.1364 1.2597]\n",
      "   [0.6657 1.2569 0.4754]]]\n",
      "\n",
      "\n",
      " [[[0.711  0.4818 0.8706]\n",
      "   [0.1162 0.268  0.8781]]\n",
      "\n",
      "  [[1.6651 1.3796 1.2738]\n",
      "   [2.2517 1.7184 0.3346]]]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "x_test_input_pool = tf.random.normal(shape=(2, 8, 8, 3))\n",
    "print('Test 1/2...')\n",
    "pool1 = MaxPool2D('swimming_pool', pool_size=(2, 2), strides=4, prev_layer_or_block=None)\n",
    "print(f'Layer has weights? {pool1.has_wts()}. Should be False.')\n",
    "pool_acts = pool1(x_test_input_pool)\n",
    "print(f'Your MaxPool2D layer net_act is:\\n{pool_acts}')\n",
    "print('It should be:')\n",
    "print('''[[[[1.5111 0.4229 0.4703]\n",
    "   [0.7936 1.0278 0.3279]]\n",
    "\n",
    "  [[2.0615 0.1364 1.2597]\n",
    "   [0.6657 1.2569 0.4754]]]\n",
    "\n",
    "\n",
    " [[[0.711  0.4818 0.8706]\n",
    "   [0.1162 0.268  0.8781]]\n",
    "\n",
    "  [[1.6651 1.3796 1.2738]\n",
    "   [2.2517 1.7184 0.3346]]]]''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2/2...\n",
      "Your MaxPool2D layer net_act is:\n",
      "[[[[2.1463 1.5458]\n",
      "   [1.2249 0.0586]]\n",
      "\n",
      "  [[1.6981 1.328 ]\n",
      "   [2.4762 1.23  ]]]]\n",
      "It should be:\n",
      "[[[[2.1463 1.5458]\n",
      "   [1.2249 0.0586]]\n",
      "\n",
      "  [[1.6981 1.328 ]\n",
      "   [2.4762 1.23  ]]]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "x_test_input_pool = tf.random.normal(shape=(1, 9, 9, 2))\n",
    "print('Test 2/2...')\n",
    "pool2 = MaxPool2D('swimming_pool', pool_size=(3, 3), strides=4, prev_layer_or_block=None)\n",
    "pool_acts = pool2(x_test_input_pool)\n",
    "print(f'Your MaxPool2D layer net_act is:\\n{pool_acts}')\n",
    "print('It should be:')\n",
    "print('''[[[[2.1463 1.5458]\n",
    "   [1.2249 0.0586]]\n",
    "\n",
    "  [[1.6981 1.328 ]\n",
    "   [2.4762 1.23  ]]]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `MaxPool2D` layer `__str__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxPool2D layer output(swimming_pool) shape: [2, 2, 2, 3]\n",
      "The above should print:\n",
      "MaxPool2D layer output(swimming_pool) shape: [2, 2, 2, 3]\n",
      "\n",
      "MaxPool2D layer output(swimming_pool) shape: [1, 2, 2, 2]\n",
      "The above should print:\n",
      "MaxPool2D layer output(swimming_pool) shape: [1, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print(pool1)\n",
    "print('The above should print:')\n",
    "print('MaxPool2D layer output(swimming_pool) shape: [2, 2, 2, 3]')\n",
    "print()\n",
    "print(pool2)\n",
    "print('The above should print:')\n",
    "print('MaxPool2D layer output(swimming_pool) shape: [1, 2, 2, 2]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2e. Implement and test `Conv2D` layer class methods\n",
    "\n",
    "These methods are in the `Conv2D` class in `layers.py`:\n",
    "- Constructor\n",
    "- `has_wts`\n",
    "- `init_params(input_shape)`\n",
    "- `compute_net_input(x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import Conv2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: Conv2D layer basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2D layer is called convoluted and has previous layer None\n",
      "You should see:\n",
      "Conv2D layer is called convoluted and has previous layer None\n",
      "Does the layer have weights? True. It should be True.\n",
      "----------Initializing wts and biases Test1/2----------\n",
      "Your wts:\n",
      "[[[[-0.0004  0.0097 -0.0111  0.003   0.0077]]\n",
      "\n",
      "  [[ 0.0082  0.0284  0.0118 -0.0047 -0.0143]]]\n",
      "\n",
      "\n",
      " [[[ 0.0049 -0.0014  0.0105  0.0115  0.0049]]\n",
      "\n",
      "  [[-0.0026  0.0006  0.0071 -0.0024  0.0181]]]] They should be:\n",
      "[[[[-0.0004  0.0097 -0.0111  0.003   0.0077]]\n",
      "\n",
      "  [[ 0.0082  0.0284  0.0118 -0.0047 -0.0143]]]\n",
      "\n",
      "\n",
      " [[[ 0.0049 -0.0014  0.0105  0.0115  0.0049]]\n",
      "\n",
      "  [[-0.0026  0.0006  0.0071 -0.0024  0.0181]]]]\n",
      "Your biases:\n",
      "[ 0.0104  0.0062  0.0153 -0.0236 -0.0161] They should be:\n",
      "[ 0.0104  0.0062  0.0153 -0.0236 -0.0161]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(2)\n",
    "x_test_input2 = tf.random.uniform(shape=(1, 2, 2, 1), minval=-2, maxval=2)\n",
    "conv = Conv2D('convoluted', 5, kernel_size=(2, 2), activation='relu', prev_layer_or_block=None, wt_scale=1e-2)\n",
    "print(f'Conv2D layer is called {conv.get_name()} and has previous layer {conv.get_prev_layer_or_block()}')\n",
    "print('You should see:')\n",
    "print('Conv2D layer is called convoluted and has previous layer None')\n",
    "print(f'Does the layer have weights? {conv.has_wts()}. It should be True.')\n",
    "\n",
    "print('----------Initializing wts and biases Test1/2----------')\n",
    "conv.init_params(input_shape=x_test_input2.shape)\n",
    "print(f'Your wts:\\n{conv.get_wts().numpy()} They should be:')\n",
    "print('''[[[[-0.0004  0.0097 -0.0111  0.003   0.0077]]\n",
    "\n",
    "  [[ 0.0082  0.0284  0.0118 -0.0047 -0.0143]]]\n",
    "\n",
    "\n",
    " [[[ 0.0049 -0.0014  0.0105  0.0115  0.0049]]\n",
    "\n",
    "  [[-0.0026  0.0006  0.0071 -0.0024  0.0181]]]]''')\n",
    "print(f'Your biases:\\n{conv.get_b().numpy()} They should be:')\n",
    "print('''[ 0.0104  0.0062  0.0153 -0.0236 -0.0161]''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Initializing wts and biases Test2/2----------\n",
      "Your wts:\n",
      "[[[[-0.011   0.0155  0.0038 -0.0088 -0.0122]\n",
      "   [-0.0098  0.0009 -0.002  -0.0056 -0.0072]]\n",
      "\n",
      "  [[-0.0063 -0.0072 -0.0035 -0.0034  0.0018]\n",
      "   [ 0.0111  0.0128 -0.0002 -0.0032  0.0037]]]\n",
      "\n",
      "\n",
      " [[[ 0.0025  0.0064  0.0215 -0.0083 -0.009 ]\n",
      "   [ 0.0139  0.0122  0.0006 -0.0049 -0.0082]]\n",
      "\n",
      "  [[-0.0019 -0.0039 -0.0066 -0.0098  0.0039]\n",
      "   [-0.0104 -0.0156 -0.0016 -0.0036 -0.002 ]]]] They should be:\n",
      "[[[[-0.011   0.0155  0.0038 -0.0088 -0.0122]\n",
      "   [-0.0098  0.0009 -0.002  -0.0056 -0.0072]]\n",
      "\n",
      "  [[-0.0063 -0.0072 -0.0035 -0.0034  0.0018]\n",
      "   [ 0.0111  0.0128 -0.0002 -0.0032  0.0037]]]\n",
      "\n",
      "\n",
      " [[[ 0.0025  0.0064  0.0215 -0.0083 -0.009 ]\n",
      "   [ 0.0139  0.0122  0.0006 -0.0049 -0.0082]]\n",
      "\n",
      "  [[-0.0019 -0.0039 -0.0066 -0.0098  0.0039]\n",
      "   [-0.0104 -0.0156 -0.0016 -0.0036 -0.002 ]]]]\n",
      "Your biases:\n",
      "[ 0.004  -0.0109 -0.0006  0.0134  0.0071] They should be:\n",
      "[ 0.004  -0.0109 -0.0006  0.0134  0.0071]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "print('----------Initializing wts and biases Test2/2----------')\n",
    "conv.init_params(input_shape=(2, 5, 5, 2))\n",
    "print(f'Your wts:\\n{conv.get_wts().numpy()} They should be:')\n",
    "print('''[[[[-0.011   0.0155  0.0038 -0.0088 -0.0122]\n",
    "   [-0.0098  0.0009 -0.002  -0.0056 -0.0072]]\n",
    "\n",
    "  [[-0.0063 -0.0072 -0.0035 -0.0034  0.0018]\n",
    "   [ 0.0111  0.0128 -0.0002 -0.0032  0.0037]]]\n",
    "\n",
    "\n",
    " [[[ 0.0025  0.0064  0.0215 -0.0083 -0.009 ]\n",
    "   [ 0.0139  0.0122  0.0006 -0.0049 -0.0082]]\n",
    "\n",
    "  [[-0.0019 -0.0039 -0.0066 -0.0098  0.0039]\n",
    "   [-0.0104 -0.0156 -0.0016 -0.0036 -0.002 ]]]]''')\n",
    "print(f'Your biases:\\n{conv.get_b().numpy()} They should be:')\n",
    "print('''[ 0.004  -0.0109 -0.0006  0.0134  0.0071]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: Conv2D layer forward pass 1/2\n",
    "\n",
    "This tests your `Conv2D` layer methods via your`__call__` implementation in `Layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your activations from Conv2D w/ ReLU:\n",
      "[[[[0.0027 0.0049 0.0064]\n",
      "   [0.     0.     0.0073]\n",
      "   [0.0081 0.0044 0.0006]]\n",
      "\n",
      "  [[0.     0.     0.    ]\n",
      "   [0.     0.0007 0.    ]\n",
      "   [0.0044 0.     0.    ]]\n",
      "\n",
      "  [[0.     0.     0.    ]\n",
      "   [0.     0.0002 0.0082]\n",
      "   [0.     0.     0.    ]]]]\n",
      "they should be:\n",
      "[[[[0.0027 0.0049 0.0064]\n",
      "   [0.     0.     0.0073]\n",
      "   [0.0081 0.0044 0.0006]]\n",
      "\n",
      "  [[0.     0.     0.    ]\n",
      "   [0.     0.0007 0.    ]\n",
      "   [0.0044 0.     0.    ]]\n",
      "\n",
      "  [[0.     0.     0.    ]\n",
      "   [0.     0.0002 0.0082]\n",
      "   [0.     0.     0.    ]]]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "conv = Conv2D('convoluted', 3, kernel_size=(3, 3), activation='relu', prev_layer_or_block=None)\n",
    "x_test_input2 = tf.random.uniform(shape=(1, 3, 3, 4), minval=-2, maxval=2)\n",
    "print('Your activations from Conv2D w/ ReLU:')\n",
    "test_acts = conv(x_test_input2)\n",
    "print(test_acts.numpy())\n",
    "print('they should be:')\n",
    "print('''[[[[0.0027 0.0049 0.0064]\n",
    "   [0.     0.     0.0073]\n",
    "   [0.0081 0.0044 0.0006]]\n",
    "\n",
    "  [[0.     0.     0.    ]\n",
    "   [0.     0.0007 0.    ]\n",
    "   [0.0044 0.     0.    ]]\n",
    "\n",
    "  [[0.     0.     0.    ]\n",
    "   [0.     0.0002 0.0082]\n",
    "   [0.     0.     0.    ]]]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: Conv2D layer forward pass 2/2\n",
    "\n",
    "This tests lazy initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your activations from Dense_2 w/ linear:\n",
      "[[[[ 0.0027  0.0049  0.0064]\n",
      "   [-0.0061  0.      0.0073]\n",
      "   [ 0.0081  0.0044  0.0006]]\n",
      "\n",
      "  [[-0.0024 -0.0052 -0.01  ]\n",
      "   [-0.0005  0.0007 -0.001 ]\n",
      "   [ 0.0044 -0.0066 -0.004 ]]\n",
      "\n",
      "  [[-0.0054 -0.0025 -0.005 ]\n",
      "   [-0.0033  0.0002  0.0082]\n",
      "   [-0.006  -0.0028 -0.0062]]]]\n",
      "they should be:\n",
      "[[[[ 0.0027  0.0049  0.0064]\n",
      "   [-0.0061  0.      0.0073]\n",
      "   [ 0.0081  0.0044  0.0006]]\n",
      "\n",
      "  [[-0.0024 -0.0052 -0.01  ]\n",
      "   [-0.0005  0.0007 -0.001 ]\n",
      "   [ 0.0044 -0.0066 -0.004 ]]\n",
      "\n",
      "  [[-0.0054 -0.0025 -0.005 ]\n",
      "   [-0.0033  0.0002  0.0082]\n",
      "   [-0.006  -0.0028 -0.0062]]]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "conv = Conv2D('convoluted', 3, kernel_size=(3, 3), activation='linear', prev_layer_or_block=None)\n",
    "x_test_input2 = tf.random.uniform(shape=(1, 3, 3, 4), minval=-2, maxval=2)\n",
    "print('Your activations from Dense_2 w/ linear:')\n",
    "for i in range(5):\n",
    "    test_acts = conv(x_test_input2)\n",
    "print(test_acts.numpy())\n",
    "print('they should be:')\n",
    "print('''[[[[ 0.0027  0.0049  0.0064]\n",
    "   [-0.0061  0.      0.0073]\n",
    "   [ 0.0081  0.0044  0.0006]]\n",
    "\n",
    "  [[-0.0024 -0.0052 -0.01  ]\n",
    "   [-0.0005  0.0007 -0.001 ]\n",
    "   [ 0.0044 -0.0066 -0.004 ]]\n",
    "\n",
    "  [[-0.0054 -0.0025 -0.005 ]\n",
    "   [-0.0033  0.0002  0.0082]\n",
    "   [-0.006  -0.0028 -0.0062]]]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `__str__` and output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2D layer output(convoluted) shape: [1, 3, 3, 3]\n",
      "The above should print:\n",
      "Conv2D layer output(convoluted) shape: [1, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(conv)\n",
    "print('The above should print:')\n",
    "print('Conv2D layer output(convoluted) shape: [1, 3, 3, 3]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Build the `DeepNetwork` and `VGG4` classes\n",
    "\n",
    "With the layers implemented, now let's tackle the network itself. This is the last step before we can start training on some data!\n",
    "\n",
    "We will divide up the job of creating a deep network into a parent and child classes:\n",
    "- `DeepNetwork` class (located in `network.py`): Serves as parent class for VGG and all the neural networks that we develop with this semester.\n",
    "- `VGG4` class (located in `vgg_nets.py`): The minimal code necessary to the `VGG4` architecture (i.e. what makes it unique vs other potential neural networks).\n",
    "\n",
    "This division of labor will allow us to rapidly build many neural networks (`VGG4`, `VGG6`, `VGG9`, etc.) with minimal added code. All the \"boilerplate\" code that every network needs like the `fit` method, will reside in the parent and be reused automatically without massive amounts of error-prone copy-pasting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import Dense\n",
    "from network import DeepNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Implement `DeepNetwork` part 1/2\n",
    "\n",
    "To help see the big picture, start implementing and testing only the following methods in `DeepNetwork`:\n",
    "\n",
    "- `DeepNetwork` constructor\n",
    "- `compile(loss, optimizer, lr, beta_1, print_summary))`: Just add the optimizer to the existing implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: Constructor and `compile`\n",
    "\n",
    "This code that will be used for most forthcoming tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(dense_out) shape: [1, 5]\n",
      "Dense layer output(dense2) shape: [1, 4]\n",
      "Dense layer output(dense1) shape: [1, 3]\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def create_test_net():\n",
    "    # Build fake layers for testing\n",
    "    test_layer1 = Dense('dense1', 3, prev_layer_or_block=None)\n",
    "    test_layer2 = Dense('dense2', 4, prev_layer_or_block=test_layer1)\n",
    "    test_layer_out = Dense('dense_out', 5, prev_layer_or_block=test_layer2)\n",
    "    # Build fake net for testing\n",
    "    test_net = DeepNetwork(input_feats_shape=(2,), reg=1.0)\n",
    "    test_net.test_layer1 = test_layer1\n",
    "    test_net.test_layer2 = test_layer2\n",
    "    test_net.output_layer = test_layer_out\n",
    "\n",
    "    def __call__(self, x):\n",
    "        net_act = self.test_layer1(x)\n",
    "        net_act = self.test_layer2(net_act)\n",
    "        net_act = self.output_layer(net_act)\n",
    "        return net_act\n",
    "\n",
    "    setattr(DeepNetwork, '__call__', __call__)\n",
    "    return test_net\n",
    "\n",
    "test_net = create_test_net()\n",
    "test_net.compile(lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the above cell should print out:\n",
    "\n",
    "\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "Dense layer output(dense_out) shape: [1, 5]\n",
    "Dense layer output(dense2) shape: [1, 4]\n",
    "Dense layer output(dense1) shape: [1, 3]\n",
    "---------------------------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network reg is 1.0 and should be 1.0.\n",
      "Input ƒeature shape is (2,) and should be (2,).\n",
      "Optimizer is adam and should be adam.\n",
      "Optimizer lr is 0.100 and should be 0.100.\n",
      "Number of parameters discovered in network is 6 and should be 6.\n"
     ]
    }
   ],
   "source": [
    "print(f'Network reg is {test_net.reg} and should be 1.0.')\n",
    "print(f'Input ƒeature shape is {test_net.input_feats_shape} and should be (2,).')\n",
    "print(f'Optimizer is {test_net.opt.name} and should be adam.')\n",
    "print(f'Optimizer lr is {float(test_net.opt.learning_rate):.3f} and should be 0.100.')\n",
    "print(f'Number of parameters discovered in network is {len(test_net.all_net_params)} and should be 6.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Implement `VGG4` network\n",
    "\n",
    "Here is an overview of the layers in the `VGG4` network:\n",
    "\n",
    "Conv2D → Conv2D → MaxPool2D → Flatten → Dense → Dropout → Dense\n",
    "\n",
    "Since the parent class `DeepNetwork` will handle training, getting predictions, and other tasks, all that needs to be done in the `VGG4` class (in `vgg_nets.py`) is implement:\n",
    "- constructor: Where you create and configure network layers and assign them to instance variables.\n",
    "- `__call__(x)`: Performs a forward pass thru your `VGG4` network with data samples `x`. If you `VGG4` network is called `vgg` and the data is called `data`, recall that you would call the `__call__` method like this: `vgg(data)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vgg_nets import VGG4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `VGG4` forward pass shapes and architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My beautiful VGG4 network!\n",
      "---------------------------------------------------------------------------\n",
      "Dense layer output(output) shape: [1, 4]\n",
      "Dropout layer output(dropout1) shape: [1, 128]\n",
      "Dense layer output(dense1) shape: [1, 128]\n",
      "Flatten layer output(flat) shape: [1, 16384]\n",
      "MaxPool2D layer output(maxpool1) shape: [1, 16, 16, 64]\n",
      "Conv2D layer output(conv2) shape: [1, 32, 32, 64]\n",
      "Conv2D layer output(conv1) shape: [1, 32, 32, 64]\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_vgg1 = VGG4(C=4, input_feats_shape=(32, 32, 3))\n",
    "print('My beautiful VGG4 network!')\n",
    "test_vgg1.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell should print out:\n",
    "\n",
    "```\n",
    "My beautiful VGG4 network!\n",
    "---------------------------------------------------------------------------\n",
    "Dense layer output(output) shape: [1, 4]\n",
    "Dropout layer output(dropout1) shape: [1, 128]\n",
    "Dense layer output(dense1) shape: [1, 128]\n",
    "Flatten layer output(flat) shape: [1, 16384]\n",
    "MaxPool2D layer output(maxpool1) shape: [1, 16, 16, 64]\n",
    "Conv2D layer output(conv2) shape: [1, 32, 32, 64]\n",
    "Conv2D layer output(conv1) shape: [1, 32, 32, 64]\n",
    "---------------------------------------------------------------------------\n",
    "```\n",
    "\n",
    "The network layer names in the parentheses probably will be different (*your chosen layer names*) and that's ok — it should have no bearing on the functionality of your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `VGG4` forward pass output\n",
    "\n",
    "Be cautious about small errors in the activations — any discrepancy may suggest a potential bug in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(output) shape: [1, 4]\n",
      "Dropout layer output(dropout1) shape: [1, 10]\n",
      "Dense layer output(dense1) shape: [1, 10]\n",
      "Flatten layer output(flat) shape: [1, 80]\n",
      "MaxPool2D layer output(maxpool1) shape: [1, 4, 4, 5]\n",
      "Conv2D layer output(conv2) shape: [1, 8, 8, 5]\n",
      "Conv2D layer output(conv1) shape: [1, 8, 8, 5]\n",
      "---------------------------------------------------------------------------\n",
      "Your output activations after the forward pass are:\n",
      "[[0.2589 0.2687 0.2406 0.2318]\n",
      " [0.2661 0.2601 0.2397 0.2342]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "test_x = tf.random.normal(shape=(2, 8, 8, 3))\n",
    "test_vgg2 = VGG4(C=4, input_feats_shape=(8, 8, 3), filters=5, dense_units=10, wt_scale=1e-1)\n",
    "test_vgg2.compile()\n",
    "test_net_act_out = test_vgg2(test_x)\n",
    "print(f'Your output activations after the forward pass are:\\n{test_net_act_out}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell should print out:\n",
    "\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "Dense layer output(output) shape: [1, 4]\n",
    "Dropout layer output(dropout1) shape: [1, 10]\n",
    "Dense layer output(dense1) shape: [1, 10]\n",
    "Flatten layer output(flat) shape: [1, 80]\n",
    "MaxPool2D layer output(maxpool1) shape: [1, 4, 4, 5]\n",
    "Conv2D layer output(conv2) shape: [1, 8, 8, 5]\n",
    "Conv2D layer output(conv1) shape: [1, 8, 8, 5]\n",
    "---------------------------------------------------------------------------\n",
    "Your output activations after the forward pass are:\n",
    "[[0.2589 0.2687 0.2406 0.2318]\n",
    " [0.2661 0.2601 0.2397 0.2342]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Implement `DeepNetwork` part 2/2\n",
    "\n",
    "Now that the `VGG4` network has been built and tested, let's make it functional so that we can train it with some data!\n",
    "\n",
    "Implement the following methods in `DeepNetwork` to finish up the class:\n",
    "- `set_layer_training_mode(is_training)`: Configures each net layer to operate in training mode or non-training mode.\n",
    "- `accuracy(y_true, y_pred)`\n",
    "- `predict(x, output_layer_net_act)`: Perform the forward pass on data samples `x` and predict their classes.\n",
    "- `loss(out_net_act, y, eps)`: Compute the general cross-entropy loss. **See equation below.**\n",
    "- `update_params(tape, loss)`: Perform one \"step\" of backprop — update the network weights and biases based on gradients recorded on the gradient tape.\n",
    "- `train_step(x_batch, y_batch)`: Do one \"step\" of training — forward backward pass.\n",
    "- `test_step(x_batch, y_batch)`: Do one \"step\" of testing/prediction.\n",
    "- `fit(x, y, x_val, y_val, batch_size, max_epochs, val_every, verbose)`: Train the deep neural network using training and (optionally) validation data.\n",
    "\n",
    "Except for one case noted in `fit`, **your code should be implemented in 100% TensorFlow** in `DeepNetwork`!\n",
    "\n",
    "#### General cross-entropy loss\n",
    "\n",
    "Here is a refresher on the equation for the general cross-entropy loss $L$ with int-coded classes $y_i$ and output layer net acts $z_i$ for sample $i$. You should implement this equation in the `loss` method.\n",
    "\n",
    "$$\n",
    "L = -\\frac{1}{N} \\sum_{i=1}^N Log \\left (z_{i, y_i} + \\epsilon \\right )\n",
    "$$\n",
    "\n",
    "The only thing new about the above equation is addition of $\\epsilon$, which is a very small fudge factor to prevent possibly taking the log of 0 in rare cases.\n",
    "\n",
    "**NOTE:** You already implemented this in CS343, so you can adapt your code. But remember, the code should be written in TensorFlow rather than NumPy here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `set_layer_training_mode`\n",
    "\n",
    "**NOTE:** The following tests go back to using the simpler network test code from Task 3a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(dense_out) shape: [1, 5]\n",
      "Dense layer output(dense2) shape: [1, 4]\n",
      "Dense layer output(dense1) shape: [1, 3]\n",
      "---------------------------------------------------------------------------\n",
      "All net layers should NOT be in training mode. Are they in training mode? False\n",
      "All net layers should be in training mode. Are they in training mode? True\n",
      "All net layers should NOT be in training mode. Are they in training mode? False\n"
     ]
    }
   ],
   "source": [
    "test_net = create_test_net()\n",
    "test_net.compile()\n",
    "\n",
    "test_net.set_layer_training_mode(False)\n",
    "should_be_false = [test_net.test_layer1.get_mode(),\n",
    "                   test_net.test_layer2.get_mode(),\n",
    "                   test_net.output_layer.get_mode()]\n",
    "\n",
    "print(f'All net layers should NOT be in training mode. Are they in training mode? {tf.reduce_any(should_be_false)}')\n",
    "\n",
    "test_net.set_layer_training_mode(True)\n",
    "should_be_true = [test_net.test_layer1.get_mode(),\n",
    "                  test_net.test_layer2.get_mode(),\n",
    "                  test_net.output_layer.get_mode()]\n",
    "\n",
    "print(f'All net layers should be in training mode. Are they in training mode? {tf.reduce_any(should_be_true)}')\n",
    "\n",
    "test_net.set_layer_training_mode(False)\n",
    "should_be_false = [test_net.test_layer1.get_mode(),\n",
    "                   test_net.test_layer2.get_mode(),\n",
    "                   test_net.output_layer.get_mode()]\n",
    "\n",
    "print(f'All net layers should NOT be in training mode. Are they in training mode? {tf.reduce_any(should_be_false)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `accuracy` and `loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your test acc is 0.7273 and it should be 0.7273.\n",
      "Your test loss is 0.4215 and it should be 0.4215.\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "test_y_true = tf.constant([1, 0, 0, 1, 2, 1, 1, 0, 0, 1, 2])\n",
    "test_y_pred = tf.constant([1, 0, 2, 1, 0, 1, 1, 0, 0, 1, 0])\n",
    "test_acc = test_net.accuracy(test_y_true, test_y_pred)\n",
    "print(f'Your test acc is {test_acc:.4f} and it should be 0.7273.')\n",
    "\n",
    "test_net_acts = tf.random.uniform(shape=(2, 5))\n",
    "test_y = tf.constant([0, 2])\n",
    "test_loss = test_net.loss(test_net_acts, test_y, eps=1e-1)\n",
    "print(f'Your test loss is {test_loss:.4f} and it should be 0.4215.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `update_params` and `train_step`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(dense_out) shape: [1, 5]\n",
      "Dense layer output(dense2) shape: [1, 4]\n",
      "Dense layer output(dense1) shape: [1, 3]\n",
      "---------------------------------------------------------------------------\n",
      "---------------------------- Before train step ----------------------------\n",
      "1st layer wts:\n",
      "[[ 0.0015  0.0004 -0.0004]\n",
      " [-0.001  -0.0012  0.0005]]\n",
      "and they should be:\n",
      "[[ 0.0015  0.0004 -0.0004]\n",
      " [-0.001  -0.0012  0.0005]]\n",
      "Output layer bias:\n",
      "[-0.0012 -0.0007 -0.0002  0.0019 -0.0005]\n",
      "and they should be:\n",
      "[-0.0012 -0.0007 -0.0002  0.0019 -0.0005]\n",
      "---------------------------- After train step ----------------------------\n",
      "1st layer wts:\n",
      "[[-9.9775 -9.9253  9.9247]\n",
      " [ 9.9693  9.9732 -9.9327]]\n",
      "and they should be:\n",
      "[[-9.9775 -9.9253  9.9247]\n",
      " [ 9.9693  9.9732 -9.9327]]\n",
      "Output layer bias:\n",
      "[-0.0012 -0.0007 -0.0002 10.0019 -0.0005]\n",
      "and they should be:\n",
      "[-0.0012 -0.0007 -0.0002 10.0019 -0.0005]\n",
      "\n",
      "The loss on the test mini-batch is 26.6442 and should be 26.6442\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)  # Make sure everyone's wts/biases are the same\n",
    "test_net = create_test_net()\n",
    "test_net.compile(lr=10.0)  # this is an insanely high lr just for testing :)\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "test_x = tf.random.uniform(shape=(3, 2))\n",
    "test_y_true = tf.constant([3, 0, 2])\n",
    "\n",
    "print(28*'-', 'Before train step', 28*'-')\n",
    "wts_0 = test_net.test_layer1.wts.numpy()\n",
    "b_2 = test_net.output_layer.b.numpy()\n",
    "print(f'1st layer wts:\\n{wts_0}')\n",
    "print('and they should be:')\n",
    "print('''[[ 0.0015  0.0004 -0.0004]\n",
    " [-0.001  -0.0012  0.0005]]''')\n",
    "print(f'Output layer bias:\\n{b_2}')\n",
    "print('and they should be:')\n",
    "print('''[-0.0012 -0.0007 -0.0002  0.0019 -0.0005]''')\n",
    "\n",
    "loss = test_net.train_step(test_x, test_y_true)\n",
    "\n",
    "print(28*'-', 'After train step', 28*'-')\n",
    "wts_0 = test_net.test_layer1.wts.numpy()\n",
    "b_2 = test_net.output_layer.b.numpy()\n",
    "print(f'1st layer wts:\\n{wts_0}')\n",
    "print('and they should be:')\n",
    "print('''[[-9.9775 -9.9253  9.9247]\n",
    " [ 9.9693  9.9732 -9.9327]]''')\n",
    "print(f'Output layer bias:\\n{b_2}')\n",
    "print('and they should be:')\n",
    "print('''[-0.0012 -0.0007 -0.0002 10.0019 -0.0005]''')\n",
    "\n",
    "print()\n",
    "print(f'The loss on the test mini-batch is {loss.numpy():.4f} and should be 26.6442')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `test_step`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(dense_out) shape: [1, 5]\n",
      "Dense layer output(dense2) shape: [1, 4]\n",
      "Dense layer output(dense1) shape: [1, 3]\n",
      "---------------------------------------------------------------------------\n",
      "Your acc is 0.200 and it should be 0.200\n",
      "Your loss is 30.723 and it should be 30.723\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)  # Make sure everyone's wts/biases are the same\n",
    "test_net = create_test_net()\n",
    "test_net.compile()\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "test_x = tf.random.uniform(shape=(5, 2))\n",
    "test_y_true = tf.constant([3, 1, 2, 1, 0])\n",
    "\n",
    "test_acc, test_loss = test_net.test_step(test_x, test_y_true)\n",
    "\n",
    "print(f'Your acc is {test_acc:.3f} and it should be 0.200')\n",
    "print(f'Your loss is {test_loss:.3f} and it should be 30.723')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Your `fit` method will be tested in the next task!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs343",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
