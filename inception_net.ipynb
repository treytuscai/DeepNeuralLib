{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR NAMES HERE**\n",
    "\n",
    "Spring 2025\n",
    "\n",
    "CS 444: Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=7)\n",
    "\n",
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 | Branch Neural Networks\n",
    "\n",
    "This project focuses on key innovations in the CNN architecture that go beyond AlexNet/VGG networks and bring us to \"modern\" CNNs that often have one or more **branches**. Our goal remains achieving high classification accuracy on image datasets. In addition to working with CIFAR-10, this week you will begin working with the CIFAR-100 dataset, which has the same number of samples as CIFAR-10, but has 100 classes.\n",
    "\n",
    "#### Week 1: Inception\n",
    "\n",
    "This notebook focuses on building and exploring the influential Inception network that won the 2014 ImageNet challenge*.\n",
    "\n",
    "**The InceptionNet that we build here has mostly the same architecture as the one that won the competition, but the hyperparameters and size are scaled down to make the training time and resources more reasonable. We are also using batch normalization throughout, which was not originally used.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Inception operations\n",
    "\n",
    "Before building out the network, let's explore the core new computational concepts that Inception introduces on their own without the complexity of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img, title):\n",
    "    '''This function is provided.'''\n",
    "    plt.imshow(img)\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Load in, view, and preprocess Miller and Allen Island test images\n",
    "\n",
    "In the cells below:\n",
    "1. Load in `miller_quad.jpg` and `allen_island.jpg`.\n",
    "2. Preprocess them: normalize them *globally* so that the minimum and maximum possible feature values are 0.0 and 1.0.\n",
    "3. Convert the images to TensorFlow tensors.\n",
    "4. Visualize/show the two preprocessed images. They should look like regular color images.\n",
    "5. Print out the shapes. They should both be `(600, 600, 3)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Miller image size:', miller_tf.shape)\n",
    "\n",
    "show_image(miller_tf, 'Miller Quad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Allen image size:', allen_tf.shape)\n",
    "\n",
    "show_image(allen_tf, 'Allen Island')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. 1x1 convolution\n",
    "\n",
    "The 1x1 convolution operation is widely used throughout InceptionNet. While it can be viewed as just a special case as regular convolution (i.e. just plug and chug with `tf.nn.conv2d` and you will get the correct answer), it offers a different interpretation than 2D convolution. We will explore this new perspective using image processing.\n",
    "\n",
    "Implement the function `conv_1x1` in `inception_ops.py` then fill in the code below to call it on the Miller image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inception_ops import conv_1x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell should use 1x1 convolution to convert the colored Miller image into grayscale, using a filter that [weights colors in proportion to which the human eye is sensitive to red, green, and blue wavelengths](https://en.wikipedia.org/wiki/Grayscale#Colorimetric_(perceptual_luminance-preserving)_conversion_to_grayscale). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One filter/neuron with 3 input color chans\n",
    "gray_filter = tf.constant([[0.299], [0.587], [0.114]])\n",
    "\n",
    "# TODO: Call conv_1x1 on your preprocessed Miller image\n",
    "\n",
    "\n",
    "print(f'The shape of miller_filtered is {miller_filtered.shape} and min/max is {tf.reduce_min(miller_filtered):.4f}/{tf.reduce_max(miller_filtered):.4f}')\n",
    "print('The shape should be (600, 600, 1) and the min/max should be 0.0000/0.9977')\n",
    "plt.imshow(miller_filtered, cmap='gray')\n",
    "plt.grid(False)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell should use 1x1 convolution to apply a [Sepia filter](https://stackoverflow.com/questions/1061093/how-is-a-sepia-tone-created) to the Miller image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 filters/neurons with 3 input color chans\n",
    "sepia_filter = tf.constant([[0.393, 0.349, 0.272],\n",
    "                            [0.769, 0.686, 0.534],\n",
    "                            [0.189, 0.168, 0.131]])\n",
    "\n",
    "# TODO: Call conv_1x1 on your preprocessed Miller image\n",
    "\n",
    "\n",
    "miller_filtered  = tf.clip_by_value(miller_filtered, clip_value_min=0, clip_value_max=1)\n",
    "\n",
    "print(f'The shape of miller_filtered is {miller_filtered.shape} and min/max is {tf.reduce_min(miller_filtered):.4f}/{tf.reduce_max(miller_filtered):.4f}')\n",
    "print('The shape should be (600, 600, 3) and the min/max should be 0.0000/1.0000')\n",
    "show_image(miller_filtered, 'Sepia Miller')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. 1x1 convolution (batch version)\n",
    "\n",
    "A key limitation of `conv_1x1` is that it only processes a single image input at a time. For this to be useful for neural networks it should be capable of processing a batch of images at a time.\n",
    "\n",
    "Address this by implementing the function `conv_1x1_batch` in `inception_ops.py` that adds mini-batch (*and stride*) support. Afterward, fill in the code below to apply the Sepia filter to both the Miller and Allen Island images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inception_ops import conv_1x1_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two images into a mini-batch\n",
    "imgs = tf.stack([miller_tf, allen_tf])\n",
    "\n",
    "# TODO: Call conv_1x1 on both images\n",
    "\n",
    "print(f'The mean of the mini-batch is {tf.reduce_mean(imgs_filtered):.4f} and should be 0.5160.')\n",
    "\n",
    "imgs_filtered  = tf.clip_by_value(imgs_filtered, clip_value_min=0, clip_value_max=1)\n",
    "\n",
    "print(f'The shape of miller_filtered is {imgs_filtered.shape} and should be (2, 600, 600, 3)')\n",
    "show_image(imgs_filtered[0], 'Sepia Miller')\n",
    "show_image(imgs_filtered[1], 'Sepia Allen Island')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, perform the same test, but use a stride of 8. If everything is working as expected, the output images should look similar to how they do above, but much blockier/pixelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Call conv_1x1 on both images\n",
    "\n",
    "print(f'The mean of the mini-batch is {tf.reduce_mean(imgs_filtered):.4f} and should be 0.5192.')\n",
    "\n",
    "imgs_filtered  = tf.clip_by_value(imgs_filtered, clip_value_min=0, clip_value_max=1)\n",
    "\n",
    "print(f'The shape of miller_filtered is {imgs_filtered.shape} and should be (2, 75, 75, 3)')\n",
    "show_image(imgs_filtered[0], 'Sepia Miller')\n",
    "show_image(imgs_filtered[1], 'Sepia Allen Island')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Build and test the Inception Block\n",
    "\n",
    "With 1x1 convolution implemented, let's build Inception Net! We will work our way up the neural network hierarchy — start by building new layers and blocks then assembling them into Inception Net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Copy files from Project 1\n",
    "\n",
    "Copy over your deep learning library from Project 1: `datasets.py`, `tf_util.py`, `layers.py`, `block.py`, `network.py`.\n",
    "\n",
    "This will allow you to reuse all your training/layer/network code that you developed in Project 1! You only need to create components that are unique and new to Inception Net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Implement `Conv2D1x1` layer\n",
    "\n",
    "This is the network layer that performs 1x1 convolution. The class inherits from `Conv2D` and is located in `inception_layers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inception_layers import Conv2D1x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `Conv2D1x1` layer\n",
    "\n",
    "(1/3) Testing weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "conv = Conv2D1x1('Test1x1', 4, do_batch_norm=False)\n",
    "conv(tf.ones([1, 4, 4, 3]))\n",
    "print(f'Your weights are\\n{conv.get_wts().numpy()}')\n",
    "print('and they should be:')\n",
    "print('''[[ 1.2337776  0.3453145 -0.3426796 -0.8459209]\n",
    " [-1.0098659  0.3839764 -0.0114104  0.9706988]\n",
    " [ 0.4919664  0.4896621 -0.5762113 -0.3535231]]''')\n",
    "print(f'Your biases are\\n{conv.get_b().numpy()}')\n",
    "print('and they should be:')\n",
    "print('''[0. 0. 0. 0.]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2/3) Testing activation and stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "conv = Conv2D1x1('Test1x1', 4, strides=2)\n",
    "tf.random.set_seed(1)\n",
    "test_net_acts = conv(tf.random.uniform([2, 4, 4, 3]))\n",
    "print(test_net_acts.numpy())\n",
    "print(f'Your net_acts are\\n{test_net_acts.numpy()}')\n",
    "print('and they should be:')\n",
    "print('''[[[[0.        0.        0.        0.       ]\n",
    "   [0.        0.        0.        0.       ]]\n",
    "\n",
    "  [[0.1286802 0.        0.        0.       ]\n",
    "   [0.1233867 0.        0.        0.       ]]]\n",
    "\n",
    "\n",
    " [[[0.        0.        0.        0.       ]\n",
    "   [0.3837539 0.        0.        0.038144 ]]\n",
    "\n",
    "  [[0.        0.        0.        0.       ]\n",
    "   [0.3536301 0.        0.        0.5435652]]]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3/3) Testing batch norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "conv = Conv2D1x1('Test1x1', 5, strides=2)\n",
    "conv(tf.ones([1, 4, 4, 3]))\n",
    "conv.init_batchnorm_params()\n",
    "\n",
    "conv.set_mode(True)\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "for i in range(3):\n",
    "    test_net_acts = conv(tf.random.uniform([2, 4, 4, 3]))\n",
    "print(f'Your net_acts after 3 training steps are\\n{test_net_acts.numpy()}')\n",
    "print('and they should be:')\n",
    "print('''[[[[1.0821913 0.7829832 0.1566384 0.        0.       ]\n",
    "   [0.        0.        0.9201921 1.5349929 1.4050572]]\n",
    "\n",
    "  [[0.        0.3684795 0.        1.1567515 1.2164567]\n",
    "   [0.2499718 0.        1.9366031 0.        0.       ]]]\n",
    "\n",
    "\n",
    " [[[0.        0.200867  0.        0.        0.0255171]\n",
    "   [0.        0.        0.0641299 0.271286  0.2673296]]\n",
    "\n",
    "  [[1.0891017 1.5742371 0.        0.1243966 0.272232 ]\n",
    "   [0.844538  0.4010135 0.        0.        0.       ]]]]''')\n",
    "\n",
    "conv.set_mode(False)\n",
    "\n",
    "test_net_acts = conv(tf.random.uniform([2, 4, 4, 3]))\n",
    "print(f'Your net_acts after transitioning to in predict mode \\n{test_net_acts.numpy()}')\n",
    "print('and they should be:')\n",
    "print('''[[[[0.0776796 0.        0.4667156 0.        0.       ]\n",
    "   [0.4479264 0.0159945 0.6306992 0.        0.       ]]\n",
    "\n",
    "  [[0.0817579 0.        0.7232838 0.        0.       ]\n",
    "   [0.0959485 0.        0.2694484 0.        0.       ]]]\n",
    "\n",
    "\n",
    " [[[0.5344409 0.0541097 0.706812  0.2156344 0.1787143]\n",
    "   [1.0073555 0.237892  0.        0.        0.       ]]\n",
    "\n",
    "  [[0.2575162 0.        1.3112392 0.        0.       ]\n",
    "   [0.9776454 0.043461  0.7792924 0.        0.       ]]]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Implement the `InceptionBlock` class \n",
    "\n",
    "This is located in `inception_block.py`.\n",
    "\n",
    "The Inception Block is the fundamental computational unit of Inception Net. Here is a refresher on its 4 branch structure:\n",
    "\n",
    "**Branch 1:** 1x1 convolution.\n",
    "\n",
    "**Branch 2:** 1x1 convolution → 3x3 2D convolution\n",
    "\n",
    "**Branch 3:** 1x1 convolution → 5x5 2D convolution\n",
    "\n",
    "**Branch 4:** 3x3 max pooling → 1x1 convolution\n",
    "\n",
    "The activations are concatenated together along the neuron dimension to form the output of the block (*i.e. think of the neurons at the end of each branch being lined up next to each other*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import Conv2D\n",
    "from inception_block import InceptionBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `InceptionBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = tf.random.Generator.from_seed(0)\n",
    "test_imgs = rng.uniform(shape=(1, 8, 8, 3))\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "in_block = InceptionBlock('testblock', 2, (3, 4), (6, 5), 7, prev_layer_or_block=None)\n",
    "test_net_acts = in_block(test_imgs)\n",
    "print(in_block)\n",
    "print(f'Shape of netActs are: {test_net_acts.shape}')\n",
    "print(f'Sum of netActs are: {tf.reduce_sum(tf.abs(test_net_acts)).numpy():.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell should print:\n",
    "\n",
    "```\n",
    "testblock:\n",
    "\tConv2D1x1 layer output(testblock/branch4_1_conv1x1) shape: [1, 8, 8, 7]\n",
    "\tMaxPool2D layer output(testblock/branch4_0_maxpool3x3) shape: [1, 8, 8, 3]\n",
    "\tConv2D layer output(testblock/branch3_1_conv5x5) shape: [1, 8, 8, 5]\n",
    "\tConv2D1x1 layer output(testblock/branch3_0_conv1x1) shape: [1, 8, 8, 6]\n",
    "\tConv2D layer output(testblock/branch2_1_conv3x3) shape: [1, 8, 8, 4]\n",
    "\tConv2D1x1 layer output(testblock/branch2_0_conv1x1) shape: [1, 8, 8, 3]\n",
    "\tConv2D1x1 layer output(testblock/branch1_0_conv1x1) shape: [1, 8, 8, 2]\n",
    "Shape of netActs are: (1, 8, 8, 18)\n",
    "Sum of netActs are: 358.5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = tf.random.Generator.from_seed(0)\n",
    "test_imgs = rng.uniform(shape=(5, 16, 16, 3))\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "conv_blah = Conv2D('convblaaa', 12, (3, 3), wt_init='he')\n",
    "in_block = InceptionBlock('testblock', 5, (6, 7), (8, 9), 2,\n",
    "                          prev_layer_or_block=conv_blah)\n",
    "test_net_acts = in_block(conv_blah(test_imgs))\n",
    "print(f'Shape of netActs are: {test_net_acts.shape} and should be (5, 16, 16, 23).')\n",
    "print(f'Sum of abs netActs is: {tf.reduce_sum(tf.abs(test_net_acts)).numpy():.1f} and should be 6726.1.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: 2D Global Average Pooling\n",
    "\n",
    "With the exception of the output layer, Inception does away with the large dense hidden layers. In its place, it uses 2D averaging pooling, a new type of net input operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. 2D Global Average Pooling\n",
    "\n",
    "Implement `global_avg_pooling_2d` in `inception_ops.py` then test it using the **luminance staircase patterns** below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inception_ops import global_avg_pooling_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to create and visualize some other test luminance staircase inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dark_staircase = np.expand_dims(np.repeat(np.linspace(0, 0.3, 6), 100), axis=1)\n",
    "dark_staircase = np.tile(dark_staircase, reps=(1, 600))\n",
    "mid_staircase = np.expand_dims(np.repeat(np.linspace(0.4, 0.6, 6), 100), axis=1)\n",
    "mid_staircase = np.tile(mid_staircase, reps=(1, 600))\n",
    "light_staircase = np.expand_dims(np.repeat(np.linspace(0.7, 1.0, 6), 100), axis=1)\n",
    "light_staircase = np.tile(light_staircase, reps=(1, 600))\n",
    "staircases_tf = tf.expand_dims(tf.stack([dark_staircase, mid_staircase, light_staircase]), axis=-1)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(12, 4))\n",
    "\n",
    "axes[0].imshow(dark_staircase, vmin=0, vmax=1, cmap='gray')\n",
    "axes[1].imshow(mid_staircase, vmin=0, vmax=1, cmap='gray')\n",
    "axes[2].imshow(light_staircase, vmin=0, vmax=1, cmap='gray')\n",
    "axes[0].set_title('staircase_1')\n",
    "axes[1].set_title('staircase_2')\n",
    "axes[2].set_title('staircase_3')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Call global_avg_pooling_2d on the luminance staircases shown above\n",
    "\n",
    "\n",
    "print(avg_pool_result.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Questions\n",
    "\n",
    "**Question 1:** Interpret what the 3 outputs means in the context of the input images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Implement `GlobalAveragePooling2D` layer\n",
    "\n",
    "This is the network layer that performs 2D average pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inception_layers import GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `GlobalAveragePooling2D` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "avgLayer = GlobalAveragePooling2D('TestAvg')\n",
    "net_acts = avgLayer(tf.random.uniform([5, 4, 4, 3]))\n",
    "print(f'Your net_acts are\\n{net_acts}')\n",
    "print('and they should be:')\n",
    "print('''[[0.3502938 0.4452531 0.6562439]\n",
    " [0.5894256 0.5440583 0.3731586]\n",
    " [0.462571  0.4454759 0.5653416]\n",
    " [0.4206027 0.5466459 0.4712621]\n",
    " [0.4241399 0.5988445 0.4847081]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Build Inception Net and train it on CIFAR-10\n",
    "\n",
    "All the building blocks for Inception Net are complete, so let's assemble the network! Here is an overview summary of the architecture:\n",
    "\n",
    "Conv2D → MaxPool2D → InceptionBlock → InceptionBlock → MaxPool2D → InceptionBlock → InceptionBlock → InceptionBlock → MaxPool2D → GlobalAveragePool2D → Dropout → Dense\n",
    "\n",
    "At the end of the task, you will train your Inception Net on CIFAR-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Assemble Inception net\n",
    "\n",
    "The `InceptionNet` class is located in `inception_net.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inception_net import InceptionNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `InceptionNet` architecture\n",
    "\n",
    "In the cell below, create a test Inception Net appropriate for CIFAR-10 then compile to see the network summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell should print:\n",
    "\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "Dense layer output(Output) shape: [1, 10]\n",
    "Dropout layer output(Dropout) shape: [1, 580]\n",
    "Global Avg Pooling 2D layer output(GlobalPool) shape: [1, 580]\n",
    "MaxPool2D layer output(MaxPool3x3_2) shape: [1, 4, 4, 580]\n",
    "Inception5:\n",
    "\tConv2D1x1 layer output(Inception5/branch4_1_conv1x1) shape: [1, 8, 8, 128]\n",
    "\tMaxPool2D layer output(Inception5/branch4_0_maxpool3x3) shape: [1, 8, 8, 320]\n",
    "\tConv2D layer output(Inception5/branch3_1_conv5x5) shape: [1, 8, 8, 128]\n",
    "\tConv2D1x1 layer output(Inception5/branch3_0_conv1x1) shape: [1, 8, 8, 64]\n",
    "\tConv2D layer output(Inception5/branch2_1_conv3x3) shape: [1, 8, 8, 196]\n",
    "\tConv2D1x1 layer output(Inception5/branch2_0_conv1x1) shape: [1, 8, 8, 128]\n",
    "\tConv2D1x1 layer output(Inception5/branch1_0_conv1x1) shape: [1, 8, 8, 128]\n",
    "Inception4:\n",
    "\tConv2D1x1 layer output(Inception4/branch4_1_conv1x1) shape: [1, 8, 8, 64]\n",
    "\tMaxPool2D layer output(Inception4/branch4_0_maxpool3x3) shape: [1, 8, 8, 320]\n",
    "\tConv2D layer output(Inception4/branch3_1_conv5x5) shape: [1, 8, 8, 64]\n",
    "\tConv2D1x1 layer output(Inception4/branch3_0_conv1x1) shape: [1, 8, 8, 32]\n",
    "\tConv2D layer output(Inception4/branch2_1_conv3x3) shape: [1, 8, 8, 128]\n",
    "\tConv2D1x1 layer output(Inception4/branch2_0_conv1x1) shape: [1, 8, 8, 96]\n",
    "\tConv2D1x1 layer output(Inception4/branch1_0_conv1x1) shape: [1, 8, 8, 64]\n",
    "Inception3:\n",
    "\tConv2D1x1 layer output(Inception3/branch4_1_conv1x1) shape: [1, 8, 8, 64]\n",
    "\tMaxPool2D layer output(Inception3/branch4_0_maxpool3x3) shape: [1, 8, 8, 320]\n",
    "\tConv2D layer output(Inception3/branch3_1_conv5x5) shape: [1, 8, 8, 64]\n",
    "\tConv2D1x1 layer output(Inception3/branch3_0_conv1x1) shape: [1, 8, 8, 32]\n",
    "\tConv2D layer output(Inception3/branch2_1_conv3x3) shape: [1, 8, 8, 128]\n",
    "\tConv2D1x1 layer output(Inception3/branch2_0_conv1x1) shape: [1, 8, 8, 96]\n",
    "\tConv2D1x1 layer output(Inception3/branch1_0_conv1x1) shape: [1, 8, 8, 64]\n",
    "MaxPool2D layer output(MaxPool3x3_1) shape: [1, 8, 8, 320]\n",
    "Inception2:\n",
    "\tConv2D1x1 layer output(Inception2/branch4_1_conv1x1) shape: [1, 16, 16, 64]\n",
    "\tMaxPool2D layer output(Inception2/branch4_0_maxpool3x3) shape: [1, 16, 16, 160]\n",
    "\tConv2D layer output(Inception2/branch3_1_conv5x5) shape: [1, 16, 16, 64]\n",
    "\tConv2D1x1 layer output(Inception2/branch3_0_conv1x1) shape: [1, 16, 16, 32]\n",
    "\tConv2D layer output(Inception2/branch2_1_conv3x3) shape: [1, 16, 16, 128]\n",
    "\tConv2D1x1 layer output(Inception2/branch2_0_conv1x1) shape: [1, 16, 16, 64]\n",
    "\tConv2D1x1 layer output(Inception2/branch1_0_conv1x1) shape: [1, 16, 16, 64]\n",
    "Inception1:\n",
    "\tConv2D1x1 layer output(Inception1/branch4_1_conv1x1) shape: [1, 16, 16, 32]\n",
    "\tMaxPool2D layer output(Inception1/branch4_0_maxpool3x3) shape: [1, 16, 16, 64]\n",
    "\tConv2D layer output(Inception1/branch3_1_conv5x5) shape: [1, 16, 16, 32]\n",
    "\tConv2D1x1 layer output(Inception1/branch3_0_conv1x1) shape: [1, 16, 16, 16]\n",
    "\tConv2D layer output(Inception1/branch2_1_conv3x3) shape: [1, 16, 16, 64]\n",
    "\tConv2D1x1 layer output(Inception1/branch2_0_conv1x1) shape: [1, 16, 16, 32]\n",
    "\tConv2D1x1 layer output(Inception1/branch1_0_conv1x1) shape: [1, 16, 16, 32]\n",
    "MaxPool2D layer output(MaxPool3x3_0) shape: [1, 16, 16, 64]\n",
    "Conv2D layer output(Conv2D_1) shape: [1, 32, 32, 64]\n",
    "---------------------------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Test Inception Net by overfitting small CIFAR-10 dev set\n",
    "\n",
    "To help test whether your Inception Net is working, train your network is overfit a small amount of data.\n",
    "\n",
    "Below:\n",
    "1. Create a dev set from the 1st 500 training CIFAR-10 samples.\n",
    "2. Train your net on the dev set for 40 epochs (turn off early stopping for this test). *Do not use any regularization.* \n",
    "\n",
    "Your training loss should start out at ~2.5 after the first epoch and rapidly plummet to 0.01 or less by about 25 epochs.\n",
    "\n",
    "**Note:** If you coded `fit` to assume there will always be a validation set present, no problem, just plug in the dev set for both the train and val sets here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c. Train Inception Net on CIFAR-10\n",
    "\n",
    "Now it is time for a real deal training run! Use default hyperparameters, except:\n",
    "- regularization of `1.5`\n",
    "- early stopping patience of `15`, learning rate patience of `4`. *Tweak these up/down slightly ast needed.*\n",
    "\n",
    "Compute and print the accuracy on the test set. Your val/test accuracy should be in the 80s.\n",
    "\n",
    "**Note:**\n",
    "- This training session should take a few hours. 1-3 minutes per epoch is reasonable. If it is much longer than that, there likely is an issue.\n",
    "- Within a few epochs (*perhaps no more than 5*), you should see the training loss decrease sharply and validation accuracy increase sharply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4d. Questions\n",
    "\n",
    "**Question 2:** How does your Inception Net compare to your best performing VGG network with respect to test set accuracy, runtime (per epoch), and learning progress (e.g. loss and/or acc progression) during training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: CIFAR-100\n",
    "\n",
    "Now that you have explored how several deep networks classify CIFAR-10, let's tackle a more challenging dataset called CIFAR-100. The CIFAR-100 dataset is similar in many ways to CIFAR-10 — it consists of 60,000 32x32 RGB natural images taken from the 80 million tiny images dataset. However, whereas CIFAR-10 has 10 classes, CIFAR-100 has 100. There are 600 images per class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Add support for loading CIFAR-100\n",
    "\n",
    "Update your `load_dataset` function in `datasets.py` to support loading in CIFAR-100 (*recommended dataset name: `'cifar100'`*). Just like CIFAR-10, the dataset is built into TensorFlow/Keras so adding support should be very quick with minimal code changes. The file `cifar100.txt` on the project website contains the human-readable string names of the classes.\n",
    "\n",
    "To make sure everything is working and to get acquainted with the dataset, load in CIFAR-100 below and make a 15x15 grid of 1st 225 training samples.\n",
    "\n",
    "**Note:** The option you added to `get_dataset` whether or not to standardize could be helpful for making your grid..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_sz = 4\n",
    "grid_sz = (15, 15)\n",
    "fig, axes = plt.subplots(nrows=grid_sz[0], ncols=grid_sz[1], figsize=(grid_sz[0]*panel_sz, grid_sz[1]*panel_sz))\n",
    "\n",
    "for r in range(grid_sz[0]):\n",
    "    for c in range(grid_sz[1]):\n",
    "        ind = r*grid_sz[0] + c\n",
    "        axes[r,c].imshow(x100_train[ind])\n",
    "        axes[r,c].set_xticks([])\n",
    "        axes[r,c].set_yticks([])\n",
    "        axes[r,c].set_title(classnames[y100_train[ind]])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Train the Inception Net on CIFAR-100\n",
    "\n",
    "See how well your Inception Net does at classifying CIFAR-100 images! Use same hyperparameters as CIFAR-10.\n",
    "\n",
    "At the end of training, print out the CIFAR-100 test accuracy.\n",
    "\n",
    "**Note:**\n",
    "- Your dataset has 100 classes now, not 10. You will need to make a small update to your network config.\n",
    "- Because CIFAR-100 has the same number of samples as CIFAR-10 and each color image sample has the same `(32, 32)` spatial resolution, training your InceptionNet on CIFAR-100 should take about the same time as it did on CIFAR-10 :)\n",
    "- Make sure you are training on the **standardized** CIFAR-100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs444",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
