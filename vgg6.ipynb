{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b56ff4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Trey Tuscai and Gordon Doore\n",
    "\n",
    "Spring 2025\n",
    "\n",
    "CS 444: Deep Learning\n",
    "\n",
    "Project 1: Deep Neural Networks \n",
    "\n",
    "#### Week 2: Training deeper networks with blocks\n",
    "\n",
    "The focus this week is on block design organizing deeper neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f22c9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 19:19:53.856203: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939a26",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 5: Building deeper neural networks with blocks\n",
    "\n",
    "In the quest to classify CIFAR-10 images with the highest accuracy possible, we would like to build a neural network that is deeper than VGG4 and has a greater capacity to learn more complex, nonlinear patterns in the images. Let's focus on designing a slightly deeper network than VGG4 that we will call VGG6 that has the following architecture:\n",
    "\n",
    "Conv2D → Conv2D → MaxPool2D → **Conv2D → Conv2D → MaxPool2D** → Flatten → *Dense → Dropout* → Dense\n",
    "\n",
    "Notice how the bold set of `Conv2D`/`MaxPool2D` layers are repeats of the layers to their left. It turns out, it may be beneficial to replicate the `Dense`/`Dropout` layers (italicized) toward the end of the network multiple times as well in deeper versions.\n",
    "\n",
    "Review your code for assembling `VGG4`. Building `VGG6` would require some copy-pasting of layer creation code. Imagine building even deeper versions with even more layers (e.g. `VGG9`) — this copy-paste process would get tedious, unwieldy, and potentially be error prone the bigger the network gets!\n",
    "\n",
    "For this reason, modern deep neural networks are often built using **blocks**: sequences of layers that repeat over and over again as you get farther into the network. For example, imagine replacing the layers **Conv2D → Conv2D → MaxPool2D** with a SINGLE new object that represents performing that sequence of those 3 layers. If we also do this for the `Dense`/`Dropout` layers, the architecture would look like:\n",
    "\n",
    "VGGConvBlock_0 → **VGGConvBlock_1** → Flatten → *VGGDenseBlock_0* → Dense\n",
    "\n",
    "Much simpler, more manageable, and easier to scale up to deeper nets!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b429f0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5a. Build and test `VGG` blocks\n",
    "\n",
    "The file `block.py` contains both the `Block` class and the `VGGConvBlock` and `VGGDenseBlock` classes referenced above. The `Block` class is the parent class to all `Block` classes (*both ones you write this week and for the rest of the semester!*) and is designed to work with `DeepNetwork`. Just like `DeepNetwork`, it contains all the \"boilerplate\" code that needs to be written for ANY block.\n",
    "\n",
    "Aside from the constructor, I am providing you with the `Block` class fully implemented :) You only need to write code that assembles the layers that belong to a block and specify how the forward pass thru them is done. Blocks can be mixed-and-matched and interspersed with regular layers! Nice!\n",
    "\n",
    "Implement and test the following classes and methods.\n",
    "\n",
    "**Block:**\n",
    "- constructor.\n",
    "\n",
    "**VGGConvBlock:**\n",
    "- constructor: What layers belong to a `VGGConvBlock` block?\n",
    "- `__call__`: How do we perform the forward pass thru the block?\n",
    "\n",
    "**VGGDenseBlock:**\n",
    "- constructor: What layers belong to a `VGGDenseBlock` block?\n",
    "- `__call__`: How do we perform the forward pass thru the block?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88a474",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from block import VGGConvBlock, VGGDenseBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e3b7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Test: `VGGConvBlock` part 1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21e783",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestBlock:\n",
      "\tMaxPool2D layer output(TestBlock/maxpool2) shape: [1, 2, 2, 5]\n",
      "\tConv2D layer output(TestBlock/conv_1) shape: [1, 4, 4, 5]\n",
      "\tConv2D layer output(TestBlock/conv_0) shape: [1, 4, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "x_test_1 = tf.random.normal(shape=(1, 4, 4, 3))\n",
    "conv_block = VGGConvBlock('TestBlock', units=5, prev_layer_or_block=None, wt_scale=1e-1)\n",
    "conv_block(x_test_1)\n",
    "print(conv_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5558",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The above should print (naming might be different):\n",
    "\n",
    "```\n",
    "TestBlock:\n",
    "\tMaxPool2D layer output(TestBlock/maxpool2) shape: [1, 2, 2, 5]\n",
    "\tConv2D layer output(TestBlock/conv1) shape: [1, 4, 4, 5]\n",
    "\tConv2D layer output(TestBlock/conv0) shape: [1, 4, 4, 5]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01935e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your block net_acts are\n",
      "[[[[0.372 0.487 0.071 0.158 0.   ]\n",
      "   [0.156 0.412 0.175 0.116 0.019]]\n",
      "\n",
      "  [[0.51  0.548 0.085 0.299 0.   ]\n",
      "   [0.375 0.327 0.169 0.209 0.   ]]]\n",
      "\n",
      "\n",
      " [[[0.25  0.551 0.    0.321 0.022]\n",
      "   [0.461 0.47  0.116 0.132 0.044]]\n",
      "\n",
      "  [[0.37  0.546 0.003 0.221 0.009]\n",
      "   [0.37  0.486 0.123 0.054 0.   ]]]]\n",
      "and they should be:\n",
      "[[[[0.372 0.487 0.071 0.158 0.   ]\n",
      "   [0.156 0.412 0.175 0.116 0.019]]\n",
      "\n",
      "  [[0.51  0.548 0.085 0.299 0.   ]\n",
      "   [0.375 0.327 0.169 0.209 0.   ]]]\n",
      "\n",
      "\n",
      " [[[0.25  0.551 0.    0.321 0.022]\n",
      "   [0.461 0.47  0.116 0.132 0.044]]\n",
      "\n",
      "  [[0.37  0.546 0.003 0.221 0.009]\n",
      "   [0.37  0.486 0.123 0.054 0.   ]]]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "x_test_2 = tf.random.normal(shape=(2, 4, 4, 3))\n",
    "acts = conv_block(x_test_2)\n",
    "print(f'Your block net_acts are\\n{acts.numpy()}')\n",
    "print('and they should be:')\n",
    "print('''[[[[0.372 0.487 0.071 0.158 0.   ]\n",
    "   [0.156 0.412 0.175 0.116 0.019]]\n",
    "\n",
    "  [[0.51  0.548 0.085 0.299 0.   ]\n",
    "   [0.375 0.327 0.169 0.209 0.   ]]]\n",
    "\n",
    "\n",
    " [[[0.25  0.551 0.    0.321 0.022]\n",
    "   [0.461 0.47  0.116 0.132 0.044]]\n",
    "\n",
    "  [[0.37  0.546 0.003 0.221 0.009]\n",
    "   [0.37  0.486 0.123 0.054 0.   ]]]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05077d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Test: `VGGConvBlock` part 2/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6016f3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestBlock:\n",
      "\tDropout layer output(TestBlock/dropout) shape: [1, 2, 2, 7]\n",
      "\tMaxPool2D layer output(TestBlock/maxpool2) shape: [1, 2, 2, 7]\n",
      "\tConv2D layer output(TestBlock/conv_1) shape: [1, 4, 4, 7]\n",
      "\tConv2D layer output(TestBlock/conv_0) shape: [1, 4, 4, 7]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "x_test_1 = tf.random.normal(shape=(1, 4, 4, 3))\n",
    "conv_block = VGGConvBlock('TestBlock', units=7, prev_layer_or_block=None, dropout=True)\n",
    "conv_block(x_test_1)\n",
    "print(conv_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b88cb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The above should print (naming might be different):\n",
    "\n",
    "```\n",
    "TestBlock:\n",
    "\tDropout layer output(TestBlock/dropout) shape: [1, 2, 2, 7]\n",
    "\tMaxPool2D layer output(TestBlock/maxpool2) shape: [1, 2, 2, 7]\n",
    "\tConv2D layer output(TestBlock/conv1) shape: [1, 4, 4, 7]\n",
    "\tConv2D layer output(TestBlock/conv0) shape: [1, 4, 4, 7]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "902051",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your block net_acts are\n",
      "[[[[0.002 0.002 0.    0.    0.    0.    0.   ]\n",
      "   [0.002 0.002 0.    0.    0.    0.    0.   ]]\n",
      "\n",
      "  [[0.002 0.002 0.    0.    0.    0.    0.   ]\n",
      "   [0.002 0.002 0.    0.    0.    0.    0.   ]]]]\n",
      "and they should be:\n",
      "[[[[0.002 0.002 0.    0.    0.    0.    0.   ]\n",
      "   [0.002 0.002 0.    0.    0.    0.    0.   ]]\n",
      "\n",
      "  [[0.002 0.002 0.    0.    0.    0.    0.   ]\n",
      "   [0.002 0.002 0.    0.    0.    0.    0.   ]]]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "x_test_2 = tf.random.normal(shape=(1, 4, 4, 3))\n",
    "acts = conv_block(x_test_2)\n",
    "print(f'Your block net_acts are\\n{acts.numpy()}')\n",
    "print('and they should be:')\n",
    "print('''[[[[0.002 0.002 0.    0.    0.    0.    0.   ]\n",
    "   [0.002 0.002 0.    0.    0.    0.    0.   ]]\n",
    "\n",
    "  [[0.002 0.002 0.    0.    0.    0.    0.   ]\n",
    "   [0.002 0.002 0.    0.    0.    0.    0.   ]]]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9d2f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Test: `VGGDenseBlock` part 1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abcec1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestDenseBlock:\n",
      "\tDropout layer output(TestDenseBlock/dropout) shape: [1, 2]\n",
      "\tDense layer output(TestDenseBlock/dense_0) shape: [1, 2]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "x_test_1 = tf.random.normal(shape=(1, 6))\n",
    "dense_block = VGGDenseBlock('TestDenseBlock', units=(2,), prev_layer_or_block=None, wt_scale=1e-1)\n",
    "dense_block(x_test_1)\n",
    "print(dense_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76229",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The above should print (naming might be different):\n",
    "\n",
    "```\n",
    "TestDenseBlock:\n",
    "\tDropout layer output(TestDenseBlock/dropout) shape: [1, 2]\n",
    "\tDense layer output(TestDenseBlock/dense0) shape: [1, 2]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a78afa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your block net_acts are\n",
      "[[0.   0.  ]\n",
      " [0.   0.  ]\n",
      " [0.   0.07]]\n",
      "and they should be:\n",
      "[[0.   0.  ]\n",
      " [0.   0.  ]\n",
      " [0.   0.07]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "x_test_2 = tf.random.normal(shape=(3, 6))\n",
    "acts = dense_block(x_test_2)\n",
    "print(f'Your block net_acts are\\n{acts.numpy()}')\n",
    "print('and they should be:')\n",
    "print('''[[0.   0.  ]\n",
    " [0.   0.  ]\n",
    " [0.   0.07]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c3f1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Test: `VGGDenseBlock` part 2/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a5304",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestDenseBlock:\n",
      "\tDropout layer output(TestDenseBlock/dropout) shape: [1, 5]\n",
      "\tDense layer output(TestDenseBlock/dense_1) shape: [1, 5]\n",
      "\tDropout layer output(TestDenseBlock/dropout) shape: [1, 4]\n",
      "\tDense layer output(TestDenseBlock/dense_0) shape: [1, 4]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "x_test_1 = tf.random.normal(shape=(1, 7))\n",
    "dense_block = VGGDenseBlock('TestDenseBlock', units=(4,5), prev_layer_or_block=None, num_dense_blocks=2)\n",
    "dense_block(x_test_1)\n",
    "print(dense_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b707",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The above should print (naming might be different):\n",
    "\n",
    "```\n",
    "TestDenseBlock:\n",
    "\tDropout layer output(TestDenseBlock/dropout) shape: [1, 5]\n",
    "\tDense layer output(TestDenseBlock/dense1) shape: [1, 5]\n",
    "\tDropout layer output(TestDenseBlock/dropout) shape: [1, 4]\n",
    "\tDense layer output(TestDenseBlock/dense0) shape: [1, 4]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b3002",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your block net_acts are\n",
      "[[0.002 0.002 0.    0.    0.   ]\n",
      " [0.002 0.002 0.    0.    0.   ]]\n",
      "and they should be:\n",
      "[[0.002 0.002 0.    0.    0.   ]\n",
      " [0.002 0.002 0.    0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "x_test_2 = tf.random.normal(shape=(2, 7))\n",
    "acts = dense_block(x_test_2)\n",
    "print(f'Your block net_acts are\\n{acts.numpy()}')\n",
    "print('and they should be:')\n",
    "print('''[[0.002 0.002 0.    0.    0.   ]\n",
    " [0.002 0.002 0.    0.    0.   ]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d488a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5b. Build `VGG6`\n",
    "\n",
    "Now that you have both types of VGG blocks implemented and tested, make us of them to write the `VGG6` constructor and `__call__` methods in `vgg_nets.py`. This should be a quick process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fac05",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vgg_nets import VGG6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda107",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Test: `VGG6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "900df4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(output_layer) shape: [1, 5]\n",
      "DenseBlock1:\n",
      "\tDropout layer output(DenseBlock1/dropout) shape: [1, 256]\n",
      "\tDense layer output(DenseBlock1/dense_0) shape: [1, 256]\n",
      "Flatten layer output(flat) shape: [1, 512]\n",
      "ConvBlock2:\n",
      "\tMaxPool2D layer output(ConvBlock2/maxpool2) shape: [1, 2, 2, 128]\n",
      "\tConv2D layer output(ConvBlock2/conv_1) shape: [1, 4, 4, 128]\n",
      "\tConv2D layer output(ConvBlock2/conv_0) shape: [1, 4, 4, 128]\n",
      "ConvBlock1:\n",
      "\tMaxPool2D layer output(ConvBlock1/maxpool2) shape: [1, 4, 4, 64]\n",
      "\tConv2D layer output(ConvBlock1/conv_1) shape: [1, 8, 8, 64]\n",
      "\tConv2D layer output(ConvBlock1/conv_0) shape: [1, 8, 8, 64]\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_vgg6_0 = VGG6(C=5, input_feats_shape=(8, 8, 3))\n",
    "test_vgg6_0.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05616e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The above should print something like (*layer/block names may be different and that's ok*):\n",
    "\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "Dense layer output(output) shape: [1, 5]\n",
    "DenseBlock1:\n",
    "\tDropout layer output(DenseBlock1/dropout) shape: [1, 256]\n",
    "\tDense layer output(DenseBlock1/dense0) shape: [1, 256]\n",
    "Flatten layer output(flat) shape: [1, 512]\n",
    "ConvBlock2:\n",
    "\tMaxPool2D layer output(ConvBlock2/maxpool2) shape: [1, 2, 2, 128]\n",
    "\tConv2D layer output(ConvBlock2/conv1) shape: [1, 4, 4, 128]\n",
    "\tConv2D layer output(ConvBlock2/conv0) shape: [1, 4, 4, 128]\n",
    "ConvBlock1:\n",
    "\tMaxPool2D layer output(ConvBlock1/maxpool2) shape: [1, 4, 4, 64]\n",
    "\tConv2D layer output(ConvBlock1/conv1) shape: [1, 8, 8, 64]\n",
    "\tConv2D layer output(ConvBlock1/conv0) shape: [1, 8, 8, 64]\n",
    "---------------------------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "033afd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your VGG6 output layer net_acts are\n",
      "[[0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    0.999 0.    0.001]\n",
      " [0.    0.    0.652 0.003 0.345]]\n",
      "and they should be:\n",
      "[[0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    0.999 0.    0.001]\n",
      " [0.    0.    0.652 0.003 0.345]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "x_test_3 = tf.random.normal(shape=(6, 8, 8, 3))\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "test_vgg6 = VGG6(C=5, input_feats_shape=(8, 8, 3), wt_scale=1e-1)\n",
    "acts = test_vgg6(x_test_3)\n",
    "print(f'Your VGG6 output layer net_acts are\\n{acts.numpy()}')\n",
    "print('and they should be:')\n",
    "print('''[[0.    0.    1.    0.    0.   ]\n",
    " [0.    0.    1.    0.    0.   ]\n",
    " [0.    0.    1.    0.    0.   ]\n",
    " [0.    0.    1.    0.    0.   ]\n",
    " [0.    0.    0.999 0.    0.001]\n",
    " [0.    0.    0.652 0.003 0.345]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0da2d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5c. Train `VGG6` on CIFAR-10 with the default learning rate\n",
    "\n",
    "In the cells below:\n",
    "1. Load in the CIFAR-10 dataset.\n",
    "2. Train for `25` epochs with default lr and other default hyperparameters. Your initial training and val losses should be 2.30 and should hold steady.\n",
    "3. Print out the final test accuracy.\n",
    "\n",
    "#### Important notes\n",
    "\n",
    "#### 1. Running on CoCalc and GPU\n",
    "\n",
    "You should do this training session (and all subsequent \"real\" training sessions this semester on the GPU in CoCalc). Training at this point on your CPU is basically infeasible (*feel free to try it!*).\n",
    "\n",
    "#### 2. JIT compiling the train and test steps\n",
    "\n",
    "While training VGG6 on the GPU should take ~15 secs per epoch, which is not too bad, soon deeper networks and larger datasets will make the training too slow for us (*even on the GPU!*). To speed things up considerably now and going forward, use the process we discussed in class to decorate `train_step` and `test_step` with `@tf.function(jit_compile=True)`. The 1st epoch might be a little slow, but subsequent epochs should now fly by.\n",
    "\n",
    "**Note:**\n",
    "- If you have have trouble just-in-time (JIT) compiling the train and test steps, you should be able to decorate with `@tf.function` to statically compile the network (non-JIT). This may be slower than JIT compiling the network, but should still be faster than no compilation. **If JIT compiling does not work, please seek help. JIT compiling on CoCalc will be very helpful going forward.**\n",
    "- If you are training locally on macOS, JIT compiling will not work, but falling back to `@tf.function` should work fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b0813",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your training set data have shape (45000, 32, 32, 3)\n",
      "Your training set labels have shape (45000,)\n",
      "Your val set data have shape (5000, 32, 32, 3)\n",
      "Your val set labels have shape (5000,)\n",
      "Your test set data have shape (10000, 32, 32, 3)\n",
      "Your test set labels have shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "from datasets import get_dataset\n",
    "x_train, y_train, x_val, y_val, x_test, y_test, classnames = get_dataset(\"cifar10\")\n",
    "print(f'Your training set data have shape {x_train.shape}')\n",
    "print(f'Your training set labels have shape {y_train.shape}')\n",
    "print(f'Your val set data have shape {x_val.shape}')\n",
    "print(f'Your val set labels have shape {y_val.shape}')\n",
    "print(f'Your test set data have shape {x_test.shape}')\n",
    "print(f'Your test set labels have shape {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f91e73",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(output_layer) shape: [1, 10]\n",
      "DenseBlock1:\n",
      "\tDropout layer output(DenseBlock1/dropout) shape: [1, 256]\n",
      "\tDense layer output(DenseBlock1/dense_0) shape: [1, 256]\n",
      "Flatten layer output(flat) shape: [1, 8192]\n",
      "ConvBlock2:\n",
      "\tMaxPool2D layer output(ConvBlock2/maxpool2) shape: [1, 8, 8, 128]\n",
      "\tConv2D layer output(ConvBlock2/conv_1) shape: [1, 16, 16, 128]\n",
      "\tConv2D layer output(ConvBlock2/conv_0) shape: [1, 16, 16, 128]\n",
      "ConvBlock1:\n",
      "\tMaxPool2D layer output(ConvBlock1/maxpool2) shape: [1, 16, 16, 64]\n",
      "\tConv2D layer output(ConvBlock1/conv_1) shape: [1, 32, 32, 64]\n",
      "\tConv2D layer output(ConvBlock1/conv_0) shape: [1, 32, 32, 64]\n",
      "---------------------------------------------------------------------------\n",
      "Epoch 1: Training Loss = 2.3025, Validation Loss = 2.3029, Validation Accuracy = 0.1406\n",
      "Epoch 1/25 took 3.7292 seconds\n",
      "Epoch 2: Training Loss = 2.3018, Validation Loss = 2.3034, Validation Accuracy = 0.1406\n",
      "Epoch 2/25 took 1.7920 seconds\n",
      "Epoch 3: Training Loss = 2.2975, Validation Loss = 2.3079, Validation Accuracy = 0.1406\n",
      "Epoch 3/25 took 1.7684 seconds\n",
      "Epoch 4: Training Loss = 2.2953, Validation Loss = 2.2999, Validation Accuracy = 0.1406\n",
      "Epoch 4/25 took 2.0415 seconds\n",
      "Epoch 5: Training Loss = 2.2920, Validation Loss = 2.3007, Validation Accuracy = 0.1406\n",
      "Epoch 5/25 took 1.8045 seconds\n",
      "Epoch 6: Training Loss = 2.2843, Validation Loss = 2.3064, Validation Accuracy = 0.1406\n",
      "Epoch 6/25 took 1.7786 seconds\n",
      "Epoch 7: Training Loss = 2.2925, Validation Loss = 2.3061, Validation Accuracy = 0.1406\n",
      "Epoch 7/25 took 1.9419 seconds\n",
      "Epoch 8: Training Loss = 2.3002, Validation Loss = 2.2996, Validation Accuracy = 0.1406\n",
      "Epoch 8/25 took 1.9893 seconds\n",
      "Epoch 9: Training Loss = 2.2795, Validation Loss = 2.2968, Validation Accuracy = 0.1406\n",
      "Epoch 9/25 took 1.8390 seconds\n",
      "Epoch 10: Training Loss = 2.2566, Validation Loss = 2.2776, Validation Accuracy = 0.1406\n",
      "Epoch 10/25 took 1.9548 seconds\n",
      "Epoch 11: Training Loss = 2.2507, Validation Loss = 2.2496, Validation Accuracy = 0.1406\n",
      "Epoch 11/25 took 1.9686 seconds\n",
      "Epoch 12: Training Loss = 2.2584, Validation Loss = 2.2333, Validation Accuracy = 0.1406\n",
      "Epoch 12/25 took 1.9593 seconds\n",
      "Epoch 13: Training Loss = 2.2422, Validation Loss = 2.1563, Validation Accuracy = 0.1406\n",
      "Epoch 13/25 took 2.0549 seconds\n",
      "Epoch 14: Training Loss = 2.1630, Validation Loss = 2.1021, Validation Accuracy = 0.1406\n",
      "Epoch 14/25 took 2.1802 seconds\n",
      "Epoch 15: Training Loss = 2.1776, Validation Loss = 2.1330, Validation Accuracy = 0.1562\n",
      "Epoch 15/25 took 1.9441 seconds\n",
      "Epoch 16: Training Loss = 2.1643, Validation Loss = 2.0925, Validation Accuracy = 0.2344\n",
      "Epoch 16/25 took 1.7918 seconds\n",
      "Epoch 17: Training Loss = 2.1221, Validation Loss = 2.0477, Validation Accuracy = 0.2969\n",
      "Epoch 17/25 took 1.8793 seconds\n",
      "Epoch 18: Training Loss = 2.0905, Validation Loss = 1.9882, Validation Accuracy = 0.2812\n",
      "Epoch 18/25 took 1.9530 seconds\n",
      "Epoch 19: Training Loss = 2.0890, Validation Loss = 1.9850, Validation Accuracy = 0.3750\n",
      "Epoch 19/25 took 2.0445 seconds\n",
      "Epoch 20: Training Loss = 2.1004, Validation Loss = 1.9468, Validation Accuracy = 0.3594\n",
      "Epoch 20/25 took 1.9646 seconds\n",
      "Epoch 21: Training Loss = 2.0233, Validation Loss = 1.9947, Validation Accuracy = 0.2969\n",
      "Epoch 21/25 took 2.0342 seconds\n",
      "Epoch 22: Training Loss = 2.0027, Validation Loss = 2.0932, Validation Accuracy = 0.2500\n",
      "Epoch 22/25 took 2.1476 seconds\n",
      "Epoch 23: Training Loss = 2.0029, Validation Loss = 1.9936, Validation Accuracy = 0.3750\n",
      "Epoch 23/25 took 2.2053 seconds\n",
      "Epoch 24: Training Loss = 1.9363, Validation Loss = 1.9456, Validation Accuracy = 0.3750\n",
      "Epoch 24/25 took 2.3364 seconds\n",
      "Epoch 25: Training Loss = 1.8970, Validation Loss = 1.9486, Validation Accuracy = 0.3125\n",
      "Epoch 25/25 took 2.1405 seconds\n",
      "Finished training after 25 epochs!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([2.3025045,\n",
       "  2.3017962,\n",
       "  2.2975383,\n",
       "  2.295264,\n",
       "  2.292026,\n",
       "  2.284318,\n",
       "  2.2924695,\n",
       "  2.30022,\n",
       "  2.2795436,\n",
       "  2.2565517,\n",
       "  2.2506785,\n",
       "  2.2583992,\n",
       "  2.242227,\n",
       "  2.1630278,\n",
       "  2.1776357,\n",
       "  2.164261,\n",
       "  2.122084,\n",
       "  2.0905044,\n",
       "  2.089007,\n",
       "  2.1004267,\n",
       "  2.0233483,\n",
       "  2.0027087,\n",
       "  2.0029483,\n",
       "  1.9362733,\n",
       "  1.8969505],\n",
       " [2.3028915,\n",
       "  2.3034263,\n",
       "  2.3078802,\n",
       "  2.2999346,\n",
       "  2.3007228,\n",
       "  2.3063629,\n",
       "  2.3061337,\n",
       "  2.2995586,\n",
       "  2.2968054,\n",
       "  2.2776127,\n",
       "  2.2496476,\n",
       "  2.2333236,\n",
       "  2.156332,\n",
       "  2.102113,\n",
       "  2.1329782,\n",
       "  2.0924573,\n",
       "  2.0477195,\n",
       "  1.988176,\n",
       "  1.9850364,\n",
       "  1.9468114,\n",
       "  1.9946582,\n",
       "  2.093193,\n",
       "  1.9935925,\n",
       "  1.9455557,\n",
       "  1.9485731],\n",
       " [0.140625,\n",
       "  0.140625,\n",
       "  0.140625,\n",
       "  0.140625,\n",
       "  0.140625,\n",
       "  0.140625,\n",
       "  0.140625,\n",
       "  0.140625,\n",
       "  0.140625,\n",
       "  0.140625,\n",
       "  0.140625,\n",
       "  0.140625,\n",
       "  0.140625,\n",
       "  0.140625,\n",
       "  0.15625,\n",
       "  0.234375,\n",
       "  0.296875,\n",
       "  0.28125,\n",
       "  0.375,\n",
       "  0.359375,\n",
       "  0.296875,\n",
       "  0.25,\n",
       "  0.375,\n",
       "  0.375,\n",
       "  0.3125],\n",
       " 25)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KEEP ME\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "model = VGG6(10, (32, 32, 3))\n",
    "model.compile()\n",
    "model.fit(x_train[:500], y_train[:500], x_val[:100], y_val[:100], max_epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb6f7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5d. Train `VGG6` on CIFAR-10 with a smaller learning rate\n",
    "\n",
    "In the cells below, repeat what you did in the previous subtask, but this time change the learning rate to `1e-5`. You should get a very different result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f792f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(output_layer) shape: [1, 10]\n",
      "DenseBlock1:\n",
      "\tDropout layer output(DenseBlock1/dropout) shape: [1, 256]\n",
      "\tDense layer output(DenseBlock1/dense_0) shape: [1, 256]\n",
      "Flatten layer output(flat) shape: [1, 8192]\n",
      "ConvBlock2:\n",
      "\tMaxPool2D layer output(ConvBlock2/maxpool2) shape: [1, 8, 8, 128]\n",
      "\tConv2D layer output(ConvBlock2/conv_1) shape: [1, 16, 16, 128]\n",
      "\tConv2D layer output(ConvBlock2/conv_0) shape: [1, 16, 16, 128]\n",
      "ConvBlock1:\n",
      "\tMaxPool2D layer output(ConvBlock1/maxpool2) shape: [1, 16, 16, 64]\n",
      "\tConv2D layer output(ConvBlock1/conv_1) shape: [1, 32, 32, 64]\n",
      "\tConv2D layer output(ConvBlock1/conv_0) shape: [1, 32, 32, 64]\n",
      "---------------------------------------------------------------------------\n",
      "Epoch 1: Training Loss = 2.3026, Validation Loss = 2.3026, Validation Accuracy = 0.1040\n",
      "Epoch 1/25 took 17.9662 seconds\n",
      "Epoch 2: Training Loss = 2.3026, Validation Loss = 2.3026, Validation Accuracy = 0.1040\n",
      "Epoch 2/25 took 16.1917 seconds\n",
      "Epoch 3: Training Loss = 2.3026, Validation Loss = 2.3026, Validation Accuracy = 0.1040\n",
      "Epoch 3/25 took 16.2042 seconds\n",
      "Epoch 4: Training Loss = 2.3026, Validation Loss = 2.3026, Validation Accuracy = 0.1040\n",
      "Epoch 4/25 took 16.1729 seconds\n",
      "Epoch 5: Training Loss = 2.3026, Validation Loss = 2.3026, Validation Accuracy = 0.1040\n",
      "Epoch 5/25 took 16.2952 seconds\n",
      "Epoch 6: Training Loss = 2.3026, Validation Loss = 2.3026, Validation Accuracy = 0.1040\n",
      "Epoch 6/25 took 16.2427 seconds\n",
      "Epoch 7: Training Loss = 2.3026, Validation Loss = 2.3026, Validation Accuracy = 0.1040\n",
      "Epoch 7/25 took 16.2235 seconds\n",
      "Epoch 8: Training Loss = 2.3026, Validation Loss = 2.3026, Validation Accuracy = 0.1040\n",
      "Epoch 8/25 took 16.4332 seconds\n",
      "Epoch 9: Training Loss = 2.3026, Validation Loss = 2.3026, Validation Accuracy = 0.0972\n",
      "Epoch 9/25 took 16.9615 seconds\n",
      "Epoch 10: Training Loss = 2.1641, Validation Loss = 2.0187, Validation Accuracy = 0.2574\n",
      "Epoch 10/25 took 16.1804 seconds\n",
      "Epoch 11: Training Loss = 1.9762, Validation Loss = 1.9431, Validation Accuracy = 0.3065\n",
      "Epoch 11/25 took 16.4849 seconds\n",
      "Epoch 12: Training Loss = 1.9249, Validation Loss = 1.8961, Validation Accuracy = 0.3233\n",
      "Epoch 12/25 took 16.2146 seconds\n",
      "Epoch 13: Training Loss = 1.8802, Validation Loss = 1.8551, Validation Accuracy = 0.3387\n",
      "Epoch 13/25 took 16.3057 seconds\n",
      "Epoch 14: Training Loss = 1.8401, Validation Loss = 1.8155, Validation Accuracy = 0.3502\n",
      "Epoch 14/25 took 16.1991 seconds\n",
      "Epoch 15: Training Loss = 1.8211, Validation Loss = 1.7839, Validation Accuracy = 0.3624\n",
      "Epoch 15/25 took 16.2241 seconds\n",
      "Epoch 16: Training Loss = 1.7881, Validation Loss = 1.7590, Validation Accuracy = 0.3668\n",
      "Epoch 16/25 took 16.1970 seconds\n",
      "Epoch 17: Training Loss = 1.7703, Validation Loss = 1.7374, Validation Accuracy = 0.3720\n",
      "Epoch 17/25 took 16.2944 seconds\n",
      "Epoch 18: Training Loss = 1.7431, Validation Loss = 1.7185, Validation Accuracy = 0.3784\n",
      "Epoch 18/25 took 16.2013 seconds\n",
      "Epoch 19: Training Loss = 1.7395, Validation Loss = 1.7022, Validation Accuracy = 0.3878\n",
      "Epoch 19/25 took 16.4908 seconds\n",
      "Epoch 20: Training Loss = 1.7123, Validation Loss = 1.6829, Validation Accuracy = 0.3912\n",
      "Epoch 20/25 took 16.2015 seconds\n",
      "Epoch 21: Training Loss = 1.7016, Validation Loss = 1.6691, Validation Accuracy = 0.3992\n",
      "Epoch 21/25 took 16.2697 seconds\n",
      "Epoch 22: Training Loss = 1.6917, Validation Loss = 1.6559, Validation Accuracy = 0.3990\n",
      "Epoch 22/25 took 16.1988 seconds\n",
      "Epoch 23: Training Loss = 1.6833, Validation Loss = 1.6446, Validation Accuracy = 0.4062\n",
      "Epoch 23/25 took 16.2092 seconds\n",
      "Epoch 24: Training Loss = 1.6692, Validation Loss = 1.6353, Validation Accuracy = 0.4103\n",
      "Epoch 24/25 took 16.2046 seconds\n",
      "Epoch 25: Training Loss = 1.6626, Validation Loss = 1.6234, Validation Accuracy = 0.4129\n",
      "Epoch 25/25 took 33.2805 seconds\n",
      "Finished training after 25 epochs!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([2.3025846,\n",
       "  2.3025942,\n",
       "  2.3025813,\n",
       "  2.3025885,\n",
       "  2.3025882,\n",
       "  2.3025885,\n",
       "  2.3025846,\n",
       "  2.3025846,\n",
       "  2.3025846,\n",
       "  2.1641214,\n",
       "  1.9762185,\n",
       "  1.9248806,\n",
       "  1.8802098,\n",
       "  1.8400693,\n",
       "  1.8211468,\n",
       "  1.7881016,\n",
       "  1.770274,\n",
       "  1.7430642,\n",
       "  1.7395278,\n",
       "  1.7123394,\n",
       "  1.701611,\n",
       "  1.6917154,\n",
       "  1.6832508,\n",
       "  1.6692432,\n",
       "  1.6626235],\n",
       " [2.3025658,\n",
       "  2.3025692,\n",
       "  2.302568,\n",
       "  2.3025723,\n",
       "  2.3025777,\n",
       "  2.3025844,\n",
       "  2.302593,\n",
       "  2.3026009,\n",
       "  2.3025737,\n",
       "  2.0187235,\n",
       "  1.9431403,\n",
       "  1.896143,\n",
       "  1.855137,\n",
       "  1.815513,\n",
       "  1.7838522,\n",
       "  1.7589654,\n",
       "  1.7374113,\n",
       "  1.7184769,\n",
       "  1.7021719,\n",
       "  1.682941,\n",
       "  1.6691012,\n",
       "  1.6558505,\n",
       "  1.6445627,\n",
       "  1.6352874,\n",
       "  1.6233541],\n",
       " [0.10396635,\n",
       "  0.10396635,\n",
       "  0.10396635,\n",
       "  0.10396635,\n",
       "  0.10396635,\n",
       "  0.10396635,\n",
       "  0.10396635,\n",
       "  0.10396635,\n",
       "  0.09715545,\n",
       "  0.25741187,\n",
       "  0.3064904,\n",
       "  0.32331732,\n",
       "  0.338742,\n",
       "  0.35016027,\n",
       "  0.36237982,\n",
       "  0.36678687,\n",
       "  0.37199518,\n",
       "  0.37840545,\n",
       "  0.3878205,\n",
       "  0.39122596,\n",
       "  0.3992388,\n",
       "  0.39903846,\n",
       "  0.40625,\n",
       "  0.41025642,\n",
       "  0.41286057],\n",
       " 25)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KEEP ME\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "model = VGG6(10, (32, 32, 3))\n",
    "model.compile(lr=1e-5)\n",
    "model.fit(x_train, y_train, x_val, y_val, max_epochs = 25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3700f3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5e. Questions\n",
    "\n",
    "**Question 4:** How does the modified learning rate compare with the default? Why do you think you observed what you did for VGG6 and not VGG4 trained on CIFAR-10 with the default lr?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a12ce",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer 4:** \n",
    "The higher learning rate (1e-3) led to faster convergence and higher accuracy for VGG6, while 1e-5 caused slow improvements due to minimal weight updates. This effect was more noticeable in VGG6 than VGG4 on CIFAR-10 because deeper networks may require larger updates to optimize effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d15c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 6: Early stopping and He/Kaiming initialization\n",
    "\n",
    "The experiment that you just ran illuminates two major issues with our training workflow:\n",
    "1. Cutting off training while the net is learning after waiting a long time at some prespecified number of epochs is frustrating. It would be nice to not have to manually set the number of training epochs as long as the net is making progress.\n",
    "2. Picking the correct lr that could make or break training is frustrating. It would be nice to have the net work well to a wide range of lr choices and number of layers.\n",
    "\n",
    "In this section, we will introduce the following techniques to combat these respective issues:\n",
    "1. Early stopping.\n",
    "2. He/Kaiming weight initialization (*next week*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73d753",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from network import DeepNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b69f3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 6a. Implement early stopping\n",
    "\n",
    "Implement the `early_stopping` method in `DeepNetwork` to determine the appropriate conditions to stop during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada9cb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Test: `early_stopping`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a4744",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping Test 1 (patience_1=5):\n",
      " Stopped after 5 iterations (should be 5 iterations).\n",
      " Recent loss history is [1.0, 2.0, 3.0, 4.0, 5.0] and should be [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "\n",
      "Early stopping Test 2 (patience_2=3):\n",
      " Stopped after 6 iterations (should be 6 iterations).\n",
      " Recent loss history is [0.29193902, 0.64250207, 0.9757855] and should be [0.29193902, 0.64250207, 0.9757855]\n",
      "\n",
      "Early stopping Test 3 (patience_3=6):\n",
      " Stopped after 9 iterations (should be 9 iterations).\n",
      " Recent loss history is\n",
      " [0.29193902, 0.64250207, 0.9757855, 0.43509948, 0.6601019, 0.60489583]\n",
      " and should be\n",
      " [0.29193902, 0.64250207, 0.9757855, 0.43509948, 0.6601019, 0.60489583]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dn = DeepNetwork((1,), 0.)\n",
    "\n",
    "# Test 1\n",
    "patience_1 = 5\n",
    "es_lost_hist_1 = []\n",
    "for iter in range(10):\n",
    "    curr_loss = float(iter)\n",
    "    es_lost_hist_1, stop = dn.early_stopping(es_lost_hist_1, curr_loss, patience=patience_1)\n",
    "\n",
    "    if stop:\n",
    "        break\n",
    "print(f'Early stopping Test 1 ({patience_1=}):\\n Stopped after {iter} iterations (should be 5 iterations).')\n",
    "print(f' Recent loss history is {es_lost_hist_1} and should be [1.0, 2.0, 3.0, 4.0, 5.0]')\n",
    "print()\n",
    "\n",
    "# Test 2\n",
    "tf.random.set_seed(1)\n",
    "patience_2 = 3\n",
    "es_lost_hist_2 = []\n",
    "test_2_loss_vals = list(tf.random.uniform(shape=(20,)).numpy())\n",
    "for iter in range(30):\n",
    "    curr_loss = test_2_loss_vals[iter]\n",
    "    es_lost_hist_2, stop = dn.early_stopping(es_lost_hist_2, curr_loss, patience=patience_2)\n",
    "\n",
    "    if stop:\n",
    "        break\n",
    "print(f'Early stopping Test 2 ({patience_2=}):\\n Stopped after {iter} iterations (should be 6 iterations).')\n",
    "print(f' Recent loss history is {es_lost_hist_2} and should be [0.29193902, 0.64250207, 0.9757855]')\n",
    "print()\n",
    "\n",
    "# Test 3\n",
    "tf.random.set_seed(1)\n",
    "patience_3 = 6\n",
    "es_lost_hist_3 = []\n",
    "test_3_loss_vals = list(tf.random.uniform(shape=(20,)).numpy())\n",
    "for iter in range(30):\n",
    "    curr_loss = test_3_loss_vals[iter]\n",
    "    es_lost_hist_3, stop = dn.early_stopping(es_lost_hist_3, curr_loss, patience=patience_3)\n",
    "\n",
    "    if stop:\n",
    "        break\n",
    "print(f'Early stopping Test 3 ({patience_3=}):\\n Stopped after {iter} iterations (should be 9 iterations).')\n",
    "print(f' Recent loss history is\\n {es_lost_hist_3}\\n and should be')\n",
    "print(' [0.29193902, 0.64250207, 0.9757855, 0.43509948, 0.6601019, 0.60489583]')\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b. Integrate early stopping into training\n",
    "\n",
    "Modify your `fit` function to support early stopping. Here are the changes to make:\n",
    "\n",
    "1. Before the training loop create an empty list to record the rolling list of recent validation loss values within the patience window of epochs.\n",
    "2. Each time the validation loss is computed, update and check the early stopping conditions. If the conditions are met, end the training early before `max_epochs` epochs is reached.\n",
    "3. Make sure you are returning as the 4th return argument the number of epochs before training ended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `fit` with early stopping\n",
    "\n",
    "The following test should end:\n",
    "- in about 10 secs.\n",
    "- after 300 epochs.\n",
    "- with final training loss of 0.04, Val loss of 0.06, Val acc of 96.00%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(TestDense) shape: [1, 3]\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 18:38:37.223782: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 1.1561, Validation Loss = 1.0572, Validation Accuracy = 0.6531\n",
      "Epoch 1/5000 took 0.6423 seconds\n",
      "Epoch 2/5000 took 0.0138 seconds\n",
      "Epoch 3/5000 took 0.0133 seconds\n",
      "Epoch 4/5000 took 0.0123 seconds\n",
      "Epoch 5/5000 took 0.0118 seconds\n",
      "Epoch 6/5000 took 0.0121 seconds\n",
      "Epoch 7/5000 took 0.0115 seconds\n",
      "Epoch 8/5000 took 0.0113 seconds\n",
      "Epoch 9/5000 took 0.0117 seconds\n",
      "Epoch 10/5000 took 0.0118 seconds\n",
      "Epoch 11/5000 took 0.0108 seconds\n",
      "Epoch 12/5000 took 0.0113 seconds\n",
      "Epoch 13/5000 took 0.0118 seconds\n",
      "Epoch 14/5000 took 0.0120 seconds\n",
      "Epoch 15/5000 took 0.0115 seconds\n",
      "Epoch 16/5000 took 0.0112 seconds\n",
      "Epoch 17/5000 took 0.0113 seconds\n",
      "Epoch 18/5000 took 0.0108 seconds\n",
      "Epoch 19/5000 took 0.0115 seconds\n",
      "Epoch 20/5000 took 0.0124 seconds\n",
      "Epoch 21/5000 took 0.0118 seconds\n",
      "Epoch 22/5000 took 0.0116 seconds\n",
      "Epoch 23/5000 took 0.0122 seconds\n",
      "Epoch 24/5000 took 0.0117 seconds\n",
      "Epoch 25/5000 took 0.0114 seconds\n",
      "Epoch 26/5000 took 0.0116 seconds\n",
      "Epoch 27/5000 took 0.0114 seconds\n",
      "Epoch 28/5000 took 0.0112 seconds\n",
      "Epoch 29/5000 took 0.0114 seconds\n",
      "Epoch 30/5000 took 0.0114 seconds\n",
      "Epoch 31/5000 took 0.0113 seconds\n",
      "Epoch 32/5000 took 0.0119 seconds\n",
      "Epoch 33/5000 took 0.0109 seconds\n",
      "Epoch 34/5000 took 0.0109 seconds\n",
      "Epoch 35/5000 took 0.0107 seconds\n",
      "Epoch 36/5000 took 0.0112 seconds\n",
      "Epoch 37/5000 took 0.0112 seconds\n",
      "Epoch 38/5000 took 0.0114 seconds\n",
      "Epoch 39/5000 took 0.0118 seconds\n",
      "Epoch 40/5000 took 0.0132 seconds\n",
      "Epoch 41/5000 took 0.0107 seconds\n",
      "Epoch 42/5000 took 0.0109 seconds\n",
      "Epoch 43/5000 took 0.0109 seconds\n",
      "Epoch 44/5000 took 0.0108 seconds\n",
      "Epoch 45/5000 took 0.0110 seconds\n",
      "Epoch 46/5000 took 0.0114 seconds\n",
      "Epoch 47/5000 took 0.0116 seconds\n",
      "Epoch 48/5000 took 0.0116 seconds\n",
      "Epoch 49/5000 took 0.0114 seconds\n",
      "Epoch 50/5000 took 0.0110 seconds\n",
      "Epoch 51/5000 took 0.0114 seconds\n",
      "Epoch 52/5000 took 0.0114 seconds\n",
      "Epoch 53/5000 took 0.0112 seconds\n",
      "Epoch 54/5000 took 0.0115 seconds\n",
      "Epoch 55/5000 took 0.0121 seconds\n",
      "Epoch 56/5000 took 0.0130 seconds\n",
      "Epoch 57/5000 took 0.0118 seconds\n",
      "Epoch 58/5000 took 0.0117 seconds\n",
      "Epoch 59/5000 took 0.0113 seconds\n",
      "Epoch 60/5000 took 0.0114 seconds\n",
      "Epoch 61/5000 took 0.0114 seconds\n",
      "Epoch 62/5000 took 0.0114 seconds\n",
      "Epoch 63/5000 took 0.0111 seconds\n",
      "Epoch 64/5000 took 0.0112 seconds\n",
      "Epoch 65/5000 took 0.0118 seconds\n",
      "Epoch 66/5000 took 0.0116 seconds\n",
      "Epoch 67/5000 took 0.0119 seconds\n",
      "Epoch 68/5000 took 0.0117 seconds\n",
      "Epoch 69/5000 took 0.0115 seconds\n",
      "Epoch 70/5000 took 0.0108 seconds\n",
      "Epoch 71/5000 took 0.0110 seconds\n",
      "Epoch 72/5000 took 0.0114 seconds\n",
      "Epoch 73/5000 took 0.0160 seconds\n",
      "Epoch 74/5000 took 0.0126 seconds\n",
      "Epoch 75/5000 took 0.0120 seconds\n",
      "Epoch 76/5000 took 0.0120 seconds\n",
      "Epoch 77/5000 took 0.0124 seconds\n",
      "Epoch 78/5000 took 0.0118 seconds\n",
      "Epoch 79/5000 took 0.0125 seconds\n",
      "Epoch 80/5000 took 0.0122 seconds\n",
      "Epoch 81/5000 took 0.0119 seconds\n",
      "Epoch 82/5000 took 0.0120 seconds\n",
      "Epoch 83/5000 took 0.0122 seconds\n",
      "Epoch 84/5000 took 0.0123 seconds\n",
      "Epoch 85/5000 took 0.0125 seconds\n",
      "Epoch 86/5000 took 0.0122 seconds\n",
      "Epoch 87/5000 took 0.0120 seconds\n",
      "Epoch 88/5000 took 0.0120 seconds\n",
      "Epoch 89/5000 took 0.0127 seconds\n",
      "Epoch 90/5000 took 0.0131 seconds\n",
      "Epoch 91/5000 took 0.0132 seconds\n",
      "Epoch 92/5000 took 0.0131 seconds\n",
      "Epoch 93/5000 took 0.0127 seconds\n",
      "Epoch 94/5000 took 0.0175 seconds\n",
      "Epoch 95/5000 took 0.0222 seconds\n",
      "Epoch 96/5000 took 0.0228 seconds\n",
      "Epoch 97/5000 took 0.0212 seconds\n",
      "Epoch 98/5000 took 0.0150 seconds\n",
      "Epoch 99/5000 took 0.0133 seconds\n",
      "Epoch 100/5000 took 0.0141 seconds\n",
      "Epoch 101: Training Loss = 0.0643, Validation Loss = 0.1000, Validation Accuracy = 0.9592\n",
      "Epoch 101/5000 took 0.0170 seconds\n",
      "Epoch 102/5000 took 0.0136 seconds\n",
      "Epoch 103/5000 took 0.0162 seconds\n",
      "Epoch 104/5000 took 0.0144 seconds\n",
      "Epoch 105/5000 took 0.0135 seconds\n",
      "Epoch 106/5000 took 0.0145 seconds\n",
      "Epoch 107/5000 took 0.0143 seconds\n",
      "Epoch 108/5000 took 0.0140 seconds\n",
      "Epoch 109/5000 took 0.0133 seconds\n",
      "Epoch 110/5000 took 0.0134 seconds\n",
      "Epoch 111/5000 took 0.0135 seconds\n",
      "Epoch 112/5000 took 0.0142 seconds\n",
      "Epoch 113/5000 took 0.0160 seconds\n",
      "Epoch 114/5000 took 0.0146 seconds\n",
      "Epoch 115/5000 took 0.0152 seconds\n",
      "Epoch 116/5000 took 0.0134 seconds\n",
      "Epoch 117/5000 took 0.0161 seconds\n",
      "Epoch 118/5000 took 0.0158 seconds\n",
      "Epoch 119/5000 took 0.0147 seconds\n",
      "Epoch 120/5000 took 0.0127 seconds\n",
      "Epoch 121/5000 took 0.0130 seconds\n",
      "Epoch 122/5000 took 0.0134 seconds\n",
      "Epoch 123/5000 took 0.0127 seconds\n",
      "Epoch 124/5000 took 0.0135 seconds\n",
      "Epoch 125/5000 took 0.0133 seconds\n",
      "Epoch 126/5000 took 0.0129 seconds\n",
      "Epoch 127/5000 took 0.0139 seconds\n",
      "Epoch 128/5000 took 0.0130 seconds\n",
      "Epoch 129/5000 took 0.0134 seconds\n",
      "Epoch 130/5000 took 0.0131 seconds\n",
      "Epoch 131/5000 took 0.0140 seconds\n",
      "Epoch 132/5000 took 0.0146 seconds\n",
      "Epoch 133/5000 took 0.0149 seconds\n",
      "Epoch 134/5000 took 0.0134 seconds\n",
      "Epoch 135/5000 took 0.0129 seconds\n",
      "Epoch 136/5000 took 0.0121 seconds\n",
      "Epoch 137/5000 took 0.0124 seconds\n",
      "Epoch 138/5000 took 0.0120 seconds\n",
      "Epoch 139/5000 took 0.0120 seconds\n",
      "Epoch 140/5000 took 0.0132 seconds\n",
      "Epoch 141/5000 took 0.0119 seconds\n",
      "Epoch 142/5000 took 0.0116 seconds\n",
      "Epoch 143/5000 took 0.0113 seconds\n",
      "Epoch 144/5000 took 0.0118 seconds\n",
      "Epoch 145/5000 took 0.0112 seconds\n",
      "Epoch 146/5000 took 0.0109 seconds\n",
      "Epoch 147/5000 took 0.0115 seconds\n",
      "Epoch 148/5000 took 0.0116 seconds\n",
      "Epoch 149/5000 took 0.0121 seconds\n",
      "Epoch 150/5000 took 0.0124 seconds\n",
      "Epoch 151/5000 took 0.0124 seconds\n",
      "Epoch 152/5000 took 0.0113 seconds\n",
      "Epoch 153/5000 took 0.0114 seconds\n",
      "Epoch 154/5000 took 0.0116 seconds\n",
      "Epoch 155/5000 took 0.0112 seconds\n",
      "Epoch 156/5000 took 0.0113 seconds\n",
      "Epoch 157/5000 took 0.0112 seconds\n",
      "Epoch 158/5000 took 0.0114 seconds\n",
      "Epoch 159/5000 took 0.0113 seconds\n",
      "Epoch 160/5000 took 0.0114 seconds\n",
      "Epoch 161/5000 took 0.0113 seconds\n",
      "Epoch 162/5000 took 0.0115 seconds\n",
      "Epoch 163/5000 took 0.0112 seconds\n",
      "Epoch 164/5000 took 0.0112 seconds\n",
      "Epoch 165/5000 took 0.0111 seconds\n",
      "Epoch 166/5000 took 0.0117 seconds\n",
      "Epoch 167/5000 took 0.0116 seconds\n",
      "Epoch 168/5000 took 0.0119 seconds\n",
      "Epoch 169/5000 took 0.0116 seconds\n",
      "Epoch 170/5000 took 0.0123 seconds\n",
      "Epoch 171/5000 took 0.0116 seconds\n",
      "Epoch 172/5000 took 0.0116 seconds\n",
      "Epoch 173/5000 took 0.0116 seconds\n",
      "Epoch 174/5000 took 0.0114 seconds\n",
      "Epoch 175/5000 took 0.0113 seconds\n",
      "Epoch 176/5000 took 0.0114 seconds\n",
      "Epoch 177/5000 took 0.0113 seconds\n",
      "Epoch 178/5000 took 0.0114 seconds\n",
      "Epoch 179/5000 took 0.0110 seconds\n",
      "Epoch 180/5000 took 0.0113 seconds\n",
      "Epoch 181/5000 took 0.0122 seconds\n",
      "Epoch 182/5000 took 0.0130 seconds\n",
      "Epoch 183/5000 took 0.0118 seconds\n",
      "Epoch 184/5000 took 0.0112 seconds\n",
      "Epoch 185/5000 took 0.0121 seconds\n",
      "Epoch 186/5000 took 0.0127 seconds\n",
      "Epoch 187/5000 took 0.0365 seconds\n",
      "Epoch 188/5000 took 0.0122 seconds\n",
      "Epoch 189/5000 took 0.0119 seconds\n",
      "Epoch 190/5000 took 0.0117 seconds\n",
      "Epoch 191/5000 took 0.0121 seconds\n",
      "Epoch 192/5000 took 0.0117 seconds\n",
      "Epoch 193/5000 took 0.0116 seconds\n",
      "Epoch 194/5000 took 0.0115 seconds\n",
      "Epoch 195/5000 took 0.0117 seconds\n",
      "Epoch 196/5000 took 0.0116 seconds\n",
      "Epoch 197/5000 took 0.0116 seconds\n",
      "Epoch 198/5000 took 0.0114 seconds\n",
      "Epoch 199/5000 took 0.0113 seconds\n",
      "Epoch 200/5000 took 0.0114 seconds\n",
      "Epoch 201: Training Loss = 0.0895, Validation Loss = 0.0768, Validation Accuracy = 0.9796\n",
      "Epoch 201/5000 took 0.0205 seconds\n",
      "Epoch 202/5000 took 0.0133 seconds\n",
      "Epoch 203/5000 took 0.0120 seconds\n",
      "Epoch 204/5000 took 0.0118 seconds\n",
      "Epoch 205/5000 took 0.0116 seconds\n",
      "Epoch 206/5000 took 0.0116 seconds\n",
      "Epoch 207/5000 took 0.0125 seconds\n",
      "Epoch 208/5000 took 0.0117 seconds\n",
      "Epoch 209/5000 took 0.0122 seconds\n",
      "Epoch 210/5000 took 0.0119 seconds\n",
      "Epoch 211/5000 took 0.0126 seconds\n",
      "Epoch 212/5000 took 0.0122 seconds\n",
      "Epoch 213/5000 took 0.0125 seconds\n",
      "Epoch 214/5000 took 0.0118 seconds\n",
      "Epoch 215/5000 took 0.0129 seconds\n",
      "Epoch 216/5000 took 0.0127 seconds\n",
      "Epoch 217/5000 took 0.0129 seconds\n",
      "Epoch 218/5000 took 0.0123 seconds\n",
      "Epoch 219/5000 took 0.0137 seconds\n",
      "Epoch 220/5000 took 0.0128 seconds\n",
      "Epoch 221/5000 took 0.0127 seconds\n",
      "Epoch 222/5000 took 0.0119 seconds\n",
      "Epoch 223/5000 took 0.0126 seconds\n",
      "Epoch 224/5000 took 0.0127 seconds\n",
      "Epoch 225/5000 took 0.0129 seconds\n",
      "Epoch 226/5000 took 0.0119 seconds\n",
      "Epoch 227/5000 took 0.0134 seconds\n",
      "Epoch 228/5000 took 0.0128 seconds\n",
      "Epoch 229/5000 took 0.0132 seconds\n",
      "Epoch 230/5000 took 0.0124 seconds\n",
      "Epoch 231/5000 took 0.0115 seconds\n",
      "Epoch 232/5000 took 0.0116 seconds\n",
      "Epoch 233/5000 took 0.0112 seconds\n",
      "Epoch 234/5000 took 0.0117 seconds\n",
      "Epoch 235/5000 took 0.0113 seconds\n",
      "Epoch 236/5000 took 0.0121 seconds\n",
      "Epoch 237/5000 took 0.0117 seconds\n",
      "Epoch 238/5000 took 0.0127 seconds\n",
      "Epoch 239/5000 took 0.0114 seconds\n",
      "Epoch 240/5000 took 0.0128 seconds\n",
      "Epoch 241/5000 took 0.0120 seconds\n",
      "Epoch 242/5000 took 0.0132 seconds\n",
      "Epoch 243/5000 took 0.0130 seconds\n",
      "Epoch 244/5000 took 0.0121 seconds\n",
      "Epoch 245/5000 took 0.0123 seconds\n",
      "Epoch 246/5000 took 0.0127 seconds\n",
      "Epoch 247/5000 took 0.0128 seconds\n",
      "Epoch 248/5000 took 0.0125 seconds\n",
      "Epoch 249/5000 took 0.0121 seconds\n",
      "Epoch 250/5000 took 0.0124 seconds\n",
      "Epoch 251/5000 took 0.0124 seconds\n",
      "Epoch 252/5000 took 0.0126 seconds\n",
      "Epoch 253/5000 took 0.0120 seconds\n",
      "Epoch 254/5000 took 0.0121 seconds\n",
      "Epoch 255/5000 took 0.0121 seconds\n",
      "Epoch 256/5000 took 0.0114 seconds\n",
      "Epoch 257/5000 took 0.0117 seconds\n",
      "Epoch 258/5000 took 0.0112 seconds\n",
      "Epoch 259/5000 took 0.0110 seconds\n",
      "Epoch 260/5000 took 0.0112 seconds\n",
      "Epoch 261/5000 took 0.0117 seconds\n",
      "Epoch 262/5000 took 0.0113 seconds\n",
      "Epoch 263/5000 took 0.0115 seconds\n",
      "Epoch 264/5000 took 0.0113 seconds\n",
      "Epoch 265/5000 took 0.0116 seconds\n",
      "Epoch 266/5000 took 0.0113 seconds\n",
      "Epoch 267/5000 took 0.0109 seconds\n",
      "Epoch 268/5000 took 0.0112 seconds\n",
      "Epoch 269/5000 took 0.0113 seconds\n",
      "Epoch 270/5000 took 0.0116 seconds\n",
      "Epoch 271/5000 took 0.0123 seconds\n",
      "Epoch 272/5000 took 0.0118 seconds\n",
      "Epoch 273/5000 took 0.0127 seconds\n",
      "Epoch 274/5000 took 0.0118 seconds\n",
      "Epoch 275/5000 took 0.0113 seconds\n",
      "Epoch 276/5000 took 0.0115 seconds\n",
      "Epoch 277/5000 took 0.0115 seconds\n",
      "Epoch 278/5000 took 0.0113 seconds\n",
      "Epoch 279/5000 took 0.0115 seconds\n",
      "Epoch 280/5000 took 0.0117 seconds\n",
      "Epoch 281/5000 took 0.0119 seconds\n",
      "Epoch 282/5000 took 0.0115 seconds\n",
      "Epoch 283/5000 took 0.0113 seconds\n",
      "Epoch 284/5000 took 0.0112 seconds\n",
      "Epoch 285/5000 took 0.0114 seconds\n",
      "Epoch 286/5000 took 0.0114 seconds\n",
      "Epoch 287/5000 took 0.0115 seconds\n",
      "Epoch 288/5000 took 0.0115 seconds\n",
      "Epoch 289/5000 took 0.0117 seconds\n",
      "Epoch 290/5000 took 0.0125 seconds\n",
      "Epoch 291/5000 took 0.0112 seconds\n",
      "Epoch 292/5000 took 0.0112 seconds\n",
      "Epoch 293/5000 took 0.0114 seconds\n",
      "Epoch 294/5000 took 0.0114 seconds\n",
      "Epoch 295/5000 took 0.0114 seconds\n",
      "Epoch 296/5000 took 0.0116 seconds\n",
      "Epoch 297/5000 took 0.0115 seconds\n",
      "Epoch 298/5000 took 0.0114 seconds\n",
      "Epoch 299/5000 took 0.0114 seconds\n",
      "Epoch 300/5000 took 0.0116 seconds\n",
      "Epoch 301: Training Loss = 0.0955, Validation Loss = 0.0775, Validation Accuracy = 0.9592\n",
      "Epoch 301/5000 took 0.0147 seconds\n",
      "Epoch 302/5000 took 0.0115 seconds\n",
      "Epoch 303/5000 took 0.0114 seconds\n",
      "Epoch 304/5000 took 0.0115 seconds\n",
      "Epoch 305/5000 took 0.0116 seconds\n",
      "Epoch 306/5000 took 0.0118 seconds\n",
      "Epoch 307/5000 took 0.0121 seconds\n",
      "Epoch 308/5000 took 0.0130 seconds\n",
      "Epoch 309/5000 took 0.0117 seconds\n",
      "Epoch 310/5000 took 0.0117 seconds\n",
      "Epoch 311/5000 took 0.0116 seconds\n",
      "Epoch 312/5000 took 0.0116 seconds\n",
      "Epoch 313/5000 took 0.0113 seconds\n",
      "Epoch 314/5000 took 0.0116 seconds\n",
      "Epoch 315/5000 took 0.0117 seconds\n",
      "Epoch 316/5000 took 0.0114 seconds\n",
      "Epoch 317/5000 took 0.0112 seconds\n",
      "Epoch 318/5000 took 0.0114 seconds\n",
      "Epoch 319/5000 took 0.0115 seconds\n",
      "Epoch 320/5000 took 0.0113 seconds\n",
      "Epoch 321/5000 took 0.0113 seconds\n",
      "Epoch 322/5000 took 0.0114 seconds\n",
      "Epoch 323/5000 took 0.0116 seconds\n",
      "Epoch 324/5000 took 0.0116 seconds\n",
      "Epoch 325/5000 took 0.0121 seconds\n",
      "Epoch 326/5000 took 0.0116 seconds\n",
      "Epoch 327/5000 took 0.0124 seconds\n",
      "Epoch 328/5000 took 0.0116 seconds\n",
      "Epoch 329/5000 took 0.0114 seconds\n",
      "Epoch 330/5000 took 0.0115 seconds\n",
      "Epoch 331/5000 took 0.0117 seconds\n",
      "Epoch 332/5000 took 0.0117 seconds\n",
      "Epoch 333/5000 took 0.0117 seconds\n",
      "Epoch 334/5000 took 0.0117 seconds\n",
      "Epoch 335/5000 took 0.0115 seconds\n",
      "Epoch 336/5000 took 0.0112 seconds\n",
      "Epoch 337/5000 took 0.0113 seconds\n",
      "Epoch 338/5000 took 0.0114 seconds\n",
      "Epoch 339/5000 took 0.0114 seconds\n",
      "Epoch 340/5000 took 0.0116 seconds\n",
      "Epoch 341/5000 took 0.0114 seconds\n",
      "Epoch 342/5000 took 0.0116 seconds\n",
      "Epoch 343/5000 took 0.0122 seconds\n",
      "Epoch 344/5000 took 0.0125 seconds\n",
      "Epoch 345/5000 took 0.0113 seconds\n",
      "Epoch 346/5000 took 0.0110 seconds\n",
      "Epoch 347/5000 took 0.0109 seconds\n",
      "Epoch 348/5000 took 0.0113 seconds\n",
      "Epoch 349/5000 took 0.0114 seconds\n",
      "Epoch 350/5000 took 0.0113 seconds\n",
      "Epoch 351/5000 took 0.0112 seconds\n",
      "Epoch 352/5000 took 0.0117 seconds\n",
      "Epoch 353/5000 took 0.0114 seconds\n",
      "Epoch 354/5000 took 0.0116 seconds\n",
      "Epoch 355/5000 took 0.0112 seconds\n",
      "Epoch 356/5000 took 0.0112 seconds\n",
      "Epoch 357/5000 took 0.0114 seconds\n",
      "Epoch 358/5000 took 0.0114 seconds\n",
      "Epoch 359/5000 took 0.0117 seconds\n",
      "Epoch 360/5000 took 0.0118 seconds\n",
      "Epoch 361/5000 took 0.0120 seconds\n",
      "Epoch 362/5000 took 0.0115 seconds\n",
      "Epoch 363/5000 took 0.0121 seconds\n",
      "Epoch 364/5000 took 0.0115 seconds\n",
      "Epoch 365/5000 took 0.0115 seconds\n",
      "Epoch 366/5000 took 0.0119 seconds\n",
      "Epoch 367/5000 took 0.0115 seconds\n",
      "Epoch 368/5000 took 0.0115 seconds\n",
      "Epoch 369/5000 took 0.0116 seconds\n",
      "Epoch 370/5000 took 0.0112 seconds\n",
      "Epoch 371/5000 took 0.0115 seconds\n",
      "Epoch 372/5000 took 0.0117 seconds\n",
      "Epoch 373/5000 took 0.0113 seconds\n",
      "Epoch 374/5000 took 0.0112 seconds\n",
      "Epoch 375/5000 took 0.0116 seconds\n",
      "Epoch 376/5000 took 0.0114 seconds\n",
      "Epoch 377/5000 took 0.0115 seconds\n",
      "Epoch 378/5000 took 0.0115 seconds\n",
      "Epoch 379/5000 took 0.0120 seconds\n",
      "Epoch 380/5000 took 0.0127 seconds\n",
      "Epoch 381/5000 took 0.0112 seconds\n",
      "Epoch 382/5000 took 0.0116 seconds\n",
      "Epoch 383/5000 took 0.0111 seconds\n",
      "Epoch 384/5000 took 0.0115 seconds\n",
      "Epoch 385/5000 took 0.0116 seconds\n",
      "Epoch 386/5000 took 0.0112 seconds\n",
      "Epoch 387/5000 took 0.0111 seconds\n",
      "Epoch 388/5000 took 0.0112 seconds\n",
      "Epoch 389/5000 took 0.0110 seconds\n",
      "Epoch 390/5000 took 0.0115 seconds\n",
      "Epoch 391/5000 took 0.0114 seconds\n",
      "Epoch 392/5000 took 0.0113 seconds\n",
      "Epoch 393/5000 took 0.0113 seconds\n",
      "Epoch 394/5000 took 0.0117 seconds\n",
      "Epoch 395/5000 took 0.0116 seconds\n",
      "Epoch 396/5000 took 0.0116 seconds\n",
      "Epoch 397/5000 took 0.0117 seconds\n",
      "Epoch 398/5000 took 0.0116 seconds\n",
      "Epoch 399/5000 took 0.0126 seconds\n",
      "Epoch 400/5000 took 0.0116 seconds\n",
      "Epoch 401: Training Loss = 0.0437, Validation Loss = 0.0882, Validation Accuracy = 0.9592\n",
      "Early stopping triggered at epoch 401\n",
      "Finished training after 401 epochs!\n",
      "---------------------------------------------------------------------------\n",
      "Iris test ended after 401 epochs with final val loss/acc of 0.09/0.96\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Quickly make a mock network for testing\n",
    "class SoftmaxNet(DeepNetwork):\n",
    "    def __init__(self, input_feats_shape, C, reg=0):\n",
    "        super().__init__(input_feats_shape, reg)\n",
    "        self.output_layer = Dense('TestDense', units=C, activation='softmax', prev_layer_or_block=None)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Load in Iris train/validation sets\n",
    "train_samps = tf.constant(np.load('data/iris/iris_train_samps.npy'), dtype=tf.float32)\n",
    "train_labels = tf.constant(np.load('data/iris/iris_train_labels.npy'), dtype=tf.int32)\n",
    "val_samps = tf.constant(np.load('data/iris/iris_val_samps.npy'), dtype=tf.float32)\n",
    "val_labels = tf.constant(np.load('data/iris/iris_val_labels.npy'), dtype=tf.int32)\n",
    "\n",
    "# Set some vars\n",
    "C = 3\n",
    "M = train_samps.shape[1]\n",
    "mini_batch_sz = 25\n",
    "lr = 1e-1\n",
    "max_epochs = 5000\n",
    "patience = 3\n",
    "val_every = 100  # how often (in epochs) we check the val loss/acc/early stopping\n",
    "\n",
    "# Create our test net\n",
    "tf.random.set_seed(0)\n",
    "slnet = SoftmaxNet((M,), C)\n",
    "slnet.compile(lr=lr)\n",
    "\n",
    "_, val_loss_hist, val_acc_hist, e = slnet.fit(train_samps, train_labels, val_samps, val_labels,\n",
    "                                              batch_size=mini_batch_sz,\n",
    "                                              max_epochs=max_epochs,\n",
    "                                              patience=patience,\n",
    "                                              val_every=val_every)\n",
    "\n",
    "print(75*'-')\n",
    "print(f'Iris test ended after {e} epochs with final val loss/acc of {val_loss_hist[-1]:.2f}/{val_acc_hist[-1]:.2f}')\n",
    "print(75*'-')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs444",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
